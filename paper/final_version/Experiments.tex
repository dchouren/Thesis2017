We have three main experimental goals:
\begin{enumerate}
	\item Prove it is possible to learn a useful image embedding function from publicly available and fuzzily labeled data
	\item Investigate, and, if possible, quantify the effects of data fuzziness on model learning rates and generalizability
	\item Investigate the importance of invariants
\end{enumerate}


We will first discuss our training of $\mathbf{V}$. Then we  evaluate the image embeddings produced by our full blended model and explore the embedding power of $\mathbf{V}$. Lastly, we  discuss what our Middlebury and time lapse datasets imply about the importance of image invariants.

Network training was done on either a Tesla K20m GPU with 5GB of memory or on a Tesla P100 GPU with 16GB of memory. Due to memory constraints, all models that run on the K20m GPU are trained with batch sizes of 6. Models trained on the P100 GPU are trained with batch sizes of 32. All training is single GPU. Generally, because the Flickr datasets contained at minimum 32000 pairs and often around 100,000 pairs, the training of $\mathbf{V}$ took approximately 2-4 days for 50 epochs on the Tesla P100 GPU. The training of $\mathbf{B}$, which done using 10-fold validation over the Wang set, required less than a day on the Tesla P100 GPU. Training done on the K20m GPU was roughly four times slower.


\section{Weakly Supervised Image Embedding}


\subsection{Training Module V}
Despite the extreme fuzziness of the labels as shown in Table \ref{table:data_fuzziness}, that the choices of architecture, optimizer, and loss function prove to be able to quickly locate local minima. Validation loss plummets without delay, often reaching inflection points in the loss curve within three training epochs, and learning typically has enough momentum to escape local minima and find deeper minima. Representative training and validation loss curves are displayed in Figure \ref{fig:loss}. Due to constraints of time and shared GPU resources, training was limited to 50 epochs. For most datasets, this resulted in a training time of approximately 3 days. As seen in Figure \ref{fig:loss}, additional training time would likely lower validation loss by a non-trivial amount, as the loss curves do not reach a fully flattened state, nor is there a conclusive split between training and validation loss.

\begin{figure}[!htbp]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.4\textwidth]{histories/2014_01_32000.png}  &       \includegraphics[width=0.4\textwidth]{histories/2014_06_32000.png}  \\
		(a) $\mathcal{P}_{11,2000,01\_2014}$ & (b) $\mathcal{P}_{11,2000,12\_2014}$\\[6pt]
		\includegraphics[width=0.4\textwidth]{histories/f2013-5_1m_user.png}  &       \includegraphics[width=0.4\textwidth]{histories/new_2015_all.png} \\
		(c) $\mathcal{P}_{11,2013-2015,user}$ & (d) $\mathcal{P}_{11,2000,2015}$\\[6pt]
	\end{tabular}
	\caption{Log loss plots for training and validation}
	\label{fig:loss}
\end{figure}

\subsection{Training Module B}

In constructing \textbf{B}, it is assumed that any blending function should be relatively simple, so \textbf{B} is limited to be at maximum three layers deep. To determine the proper widths of these layers, $L_1$ and $L_2$, a rough grid search is performed. Our grid search finds no meaningful differences between different layer widths, so the widths of $L_1$ and $L_2$ are arbitrarily chosen to be 1024 and 256 with a dropout layer with dropout rate of 0.3 between them. Because of the limited size of the Wang dataset used for training, experiments with ad-hoc manual architectures are conducted as well, such as including a residual layer for either the variant output or invariant output, or for both. None of these manual formulas showed any promise, so the model was reverted back to its simple two layer state for the computation of ranking accuracy. 


\subsection{Ranking Accuracy of Full Blended Model}
For reporting the accuracy of our full model, it would be normal to select the training method and dataset for $\mathbf{V}$ with the lowest validation loss and compute its blended test accuracy on the Wang set. But differences in validation loss are really due to the different datasets use for training, rather than as a product of different network architectures, so these validation losses are not useful for directly comparing model effectivenesses. Instead, each trained model is used in an ensemble prediction. Note that though many datasets have been mentioned, some, like the Middlebury datasets, were curated to explore the effect of image invariants, while others were intended for exploring the effect of an increase in the number of data samples on training. The final ensemble consists of the models trained on $\mathcal{P}_{11,2013-2015,user}, \mathcal{P}_{11,2000,2015}, \mathcal{P}_{11,2000,2013-2015}, \mathcal{P}_{20,2013-2015,user}$, and the monthly datasets from 2014, $\mathcal{P}_{11,2000,01\_2014}$ through $\mathcal{P}_{11,2000,12\_2014}$.

As a reminder, a triplet $t = (p, p^+, p^-)$ is considered to be properly ranked by an embedding function if and only if $f(\dot)$ if $||f(p) - f(p^+)|| < ||f(p) - f(p^-)||$.

Final prediction begins by taking the $\mathbf{V}$ networks trained on each of the datasets in the ensemble for each of the two training methods. $\mathbf{V}, \mathbf{I}, $ and $\mathbf{B}$ are triplicated, and Wang set input triplets are passed into for feature extraction. 10-fold cross validation is used. $\mathbf{B}$ begins with uninitialized weights and is trained on nine folds of the data before saving its predicted embeddings for the last fold. The ranking accuracy for this prediction fold is \textit{not} computed at this time--only the embeddings for each image in the validation fold are computed and saved. After saving predictions for the entire dataset for each of our different models, an overall ranking accuracy for triplets is computed using the average predicted embedding for each image. We achieve a final ranking accuracy of 64.20\%.

For comparison, embeddings produced by an ImageNet-trained model can serve as a baseline classifier. The baseline ranking accuracy is 60.8\%, which is a fairly impressive result given the granularity required by a large portion of the Wang set in upholding correct triplet rankings. This is another testament to the well-proven generalizability of features learned by ImageNet trained models. Our model surpasses this by a small but non-trivial margin. This is a substantial result given the context of our training methods. The distribution and variety of images present in the Flickr dataset is far different than the Google image queries which comprise the Wang set. Image queries are almost always object focused, and we had chosen the Flickr datasets based on ideas of maximizing stylistic similarities in photos. Wang et al. had achieved a ranking accuracy of 85\%, but this result is not applicable for comparison to our discussed technique because Wang et al. trained a model based on thousands of examples for each of the triplets in their test set.

One worry we have in interpreting our results is wondering if it is the invariant module $\mathbf{I}$ that is actually responsible for the successfully ranked embeddings. This can be proven to not be the case, however, when one considers that we have surpassed an ImageNet-only classifier, meaning that our variant module, in combination with the blending, contributes at least some amount to the successful embeddings. We attempt to quantify the division between the invariant and variant modules by computing the ranking accuracies of embeddings produced by $\mathbf{V}$ only. These are shown in Table \ref{table:val_loss}. The average ranking accuracy is 57.33\%, which is comfortably above a random ranking and quite close to the ImageNet baseline. This is perhaps a more impressive result than the overall accuracy, since at no point in its training does $\mathbf{V}$ see a single example from the Wang set. Taking into consideration the distance between the average Wang image and the average Flickr image implies that this result is also highly generalizable, so long as some transfer learning, such as occurred with the training of $\mathbf{B}$ directly on the Wang set, occurs. $\mathbf{V}$ has learned an image embedding function that is nearly as good as ImageNet's and with only a fraction of the cumulative training power. ImageNet is, of course, trained on over ten million strongly supervised images, and $\mathbf{V}$ is trained on at most 140,800 weakly labeled examples for 50 epochs.

\begin{table}
	\centering
	\begin{tabular}{c >{\centering\arraybackslash}m{1.5cm} >{\centering\arraybackslash}m{2cm} >{\centering\arraybackslash}m{2cm} >{\centering\arraybackslash}m{2cm} >{\centering\arraybackslash}m{2cm}}
		\toprule
		\bfseries{Dataset} &  \bfseries Epochs & \bfseries Samples & \bfseries Validation Loss & \bfseries Ranking Accuracy\\
		\midrule
		$\mathcal{P}_{11,2000,2013-2015}$ & 50& 176,000& 0.0034 & 0.5692\\
		$\mathcal{P}_{11,2000,2015}$ & 50 & 140,800& 0.0010 & 0.5682\\
		$\mathcal{P}_{11,2013-2015,user}$ & 50 & 57600& 0.3467 & 0.5889\\
		$\mathcal{P}_{11,2013-2015,tl}$ & 50 & 54400& 0.3037  & 0.5782\\
		$\mathcal{P}_{20,2013-2015,user}$ & 50 & 118,400 & 0.3327 & 0.5585\\
		$\mathcal{P}_{20,2013-2015,tl}$ & 50 &  110,400 & 0.0878  & 0.5736 \\
		$\mathcal{P}_{30,2013-2015,user,2h}$ &50  &198,400 & 0.0099  & 0.5973\\
		$\mathcal{P}_{11,2000,01\_2014}$ & 50 & 32000 &  -- & 0.5625\\
		$\mathcal{P}_{11,2000,02\_2014}$ & 50 & 32000&  -- & 0.5629\\
		$\mathcal{P}_{11,2000,03\_2014}$ & 50 & 32000 & -- & 0.5830\\
		$\mathcal{P}_{11,2000,04\_2014}$ & 50 & 32000 &  -- & 0.5686\\
		$\mathcal{P}_{11,2000,05\_62014}$ & 50 & 32000 &  -- & 0.5710\\
		$\mathcal{P}_{11,2000,06\_2014}$ & 50 & 32000&  -- & 0.5758\\
		$\mathcal{P}_{11,2000,07\_2014}$ & 50 & 32000 & --  & 0.5694\\
		$\mathcal{P}_{11,2000,08\_2014}$ & 50 & 32000 &  -- & 0.5810\\
		$\mathcal{P}_{11,2000,09\_2014}$ & 50 & 32000 &  -- & 0.5730\\
		$\mathcal{P}_{11,2000,10\_2014}$ & 50 & 32000 &  -- & 0.5651\\
		$\mathcal{P}_{11,2000,11\_2014}$ & 50 & 32000&  -- & 0.5788\\
		$\mathcal{P}_{11,2000,12\_2014}$ & 50 & 32000 &  -- &0.5673\\
		\bottomrule
	\end{tabular}
	\caption{Validation losses}
	\label{table:val_loss}
\end{table}

To confirm that $\mathbf{V}$ has learned a useful image embedding function that is not merely a subset of $\mathbf{I}$/ImageNet, we compare the overlap in their ranking predictions.

As shown in Figure \ref{fig:overlaps}, on average, our models correctly rank 61.3\% of the same triplets that the baseline classifier does. In total, we rank 57.5\% of triplets in the same fashion, counting jointly incorrectly ranked triplets as well. The relatively low overlap between our models, and the fact that this overlaps remains the same range without exception, indicates that the learnable embedding from our dataset differs to a significant degree from the one learnable from ImageNet. Given that the baseline classifier correctly ranks 60\% of triplets, our correct ranking overlaps indicates that roughly 37\% of triplets are correctly ranked by both our models and the baseline. An additional 20-25\% are ranked correctly by either the baseline model or one of our models, but not by both.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{pred_overlaps.png}
	\caption{Prediction overlaps with baseline}
	\label{fig:overlaps}
\end{figure}

\subsection{Comparison of Training Methods for V}
Recall that we train $\mathbf{V}$ using two methods, $T_c^1$, where the concatenated outputs from $\mathbf{I}$ and $\mathbf{V}$ are used to generate the contrastive loss and update $\mathbf{V}$ only, and $T_c^2$, where only the output from $\mathbf{V}$ is sent to the loss function.

In our discussion, let the intermediate embedding vector used as input to $\mathbf{B}$ be referred to as $E_c$ and let the final embedding vector outputted by $\mathbf{B}$ be referred to as $E_b$. In the case where we use the training $T_c^1$, $E_c$ is the concatenation of the invariant embedding, $E_i$, and the variant embedding, $E_v$. When discussing embedding distances among pairs or triplets, embedding with a superscript $E^+$ or $E^-$ will refer to the positive or negative example, and embeddings without superscripts will refer to the base image.

We initially expected embeddings $E_b$ produced by models trained using $T_c^1$ to outperform models trained using $T_c^2$. Either images will be similar because they contain the same type of soft style, in which case both $T_c^1$ and $T_c^2$ will pick up on this, or they will be similar despite differences in soft style because the physical composition of the images will be similar in a way approximately learnable by an ImageNet model. In the latter case, only $T_c^1$ trained embeddings will capture this. Because of the way we sample images from our Flickr set, which is entirely without a notion of class, we do not expect to learn any notion of invariance. The ImageNet training process, as discussed, strongly enforces the learning of invariance. In other words, we expect $T_c^1$ to outperform $T_c^2$ because the set of features it trains a model to detect should approximate a superset of the features $T_c^2$ will train a model to detect. We also think $T_c^1$ will force $\mathbf{V}$ to learn a set of features farther from ImageNet learnable features. Since the output of $\mathbf{I}$ is sent to the loss function, when training on two images with ImageNet-invariant features, the overall Euclidean distance of the outputted vectors to be smaller on average and thus not activate a large update. Then $\mathbf{V}$ is more likely to receive large gradients when $\mathbf{I}$ is unable to detect any common salient features.

In actuality, as seen in Table \ref{table:val_loss}, it appears that $T_c^1$ struggles to optimize on the loss space of our datasets. In theory, the addition of features should never decrease a model's effectiveness, since any weighing of the smaller set of features will be a subset of the possible weightings of the larger set, with optimization solutions being equivalent when the extra features are given weights of 0. Reaching this solution, however, might take a prohibitively long time. Perhaps more importantly, there is potential for large errors in the magnitude of our weight updates to the variant model. Should the invariant portions of our output vector be dramatically different, this will cause the overall Euclidean distance of our prediction to be large as well. But since we are only updating the variant model, we will change its weights in an unwarranted manner because of this large difference. This would be the opposite of what we had intuited would happen, with the training of $\mathbf{V}$ being driven by large gradients received in the absence of $\mathbf{I}$ detecting useful features.

Despite our intuition, it appears that the training of $\mathbf{I}$ and $\mathbf{V}$ is best left separate. These empirical results support the idea that a third network is needed to blend their outputs.


\section{Importance of Low-Level Descriptors}
Lastly, we investigated the importance of image invariants through our time lapse and Middlebury datasets. The Middlebury dataset contains just 31 unique objects, and with only four images per object, it not suited to teaching a network a wide variety of image invariants. Likewise, the time lapse dataset identifies only images of the same location, with images mainly differing in the temporal domain, as similar. Though lighting conditions change, since there is only one "class" of image in this dataset, any invariants learned are not general image invariants, such as rotational invariance for salient objects, but are unique to the structure of the time lapse base image. 

Given that these datasets are so close to the limit case of training a network with identical images labeled as similar and random images labeled as different--which we would not expect to learn any meaningful information--it is surprising to see in Table \ref{table:curated} that ranking accuracies for models trained on these datasets seems comfortably above 50\% random guessing. The positive examples in these datasets do contain very similar distributions and locations of low-level local descriptors. While this area requires further research, it is likely that the majority of the discriminative power learned by these models can be attributed to the importance of these descriptors. This would mean that the 60\% baseline accuracy achieved by ImageNet models likely owes a substantial portion of its performance to these descriptors as well.


\begin{table}
	\centering
	\begin{tabular}{c >{\centering\arraybackslash}m{1.5cm} >{\centering\arraybackslash}m{2cm} >{\centering\arraybackslash}m{2cm} >{\centering\arraybackslash}m{2cm} >{\centering\arraybackslash}m{2cm}}
		\toprule
		\bfseries{Dataset} &  \bfseries Epochs & \bfseries Samples & \bfseries Validation Loss & \bfseries Ranking Accuracy\\
		\midrule
		$\mathcal{P}_{11,2013-2015,tl}$ & 50 & 54400& 0.3037  & 0.5782\\
		$\mathcal{P}_{20,2013-2015,tl}$ & 50 &  110,400 & 0.0878  & 0.5736 \\
		$\mathcal{P}_{MiddleburyLR,diff\_user}$ & 50 & 32000 & 0.0000  & 0.5535\\
		$\mathcal{P}_{MiddleburyLR,same\_user}$ & 50  & 32000 & 0.0878 & 0.5611\\
		$\mathcal{P}_{MiddleburyLREL,diff\_user}$ & 50  & 32000 & 0.0897  & 0.5851\\
		$\mathcal{P}_{MiddleburyLREL,same\_user}$ & 50 & 32000& 0.0039  & 0.5390\\
		\bottomrule
	\end{tabular}
	\caption{Validation losses}
	\label{table:curated}
\end{table}





