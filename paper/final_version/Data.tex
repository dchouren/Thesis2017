
The success of Kraus et al.'s models despite the fuzziness of their data motivates our decision to build a corpus for image similarity and label it using our own heuristics. ImageNet is perhaps unable to advance beyond general classification and understanding because it is limited by its finite number of classes. Rather than replicate Kraus' approach and attempt to increase this number by a magnitude, we instead hope to learn an embedding function from classless data, which will better approximate the continuous nature of fine-grained image similarity.

\section{Flickr Data}\label{sec:flickr_data}
Our corpus is built by pulling all geo-tagged images uploaded to Flickr between 00:00:00 (GMT) January 1, 2006 to 00:00:00 January 1, 2017 with latitude and longitude inside the [lower left, upper right] bounding box [(-74.052544, 40.525070), (-73.740685, 40.889249)]. This bounding box roughly corresponds to the city limits of New York, New York. There are over 6 million images in the dataset, with roughly 500,000 each for the years from 2009-2016. In addition to the latitude and longitude, each image was downloaded with an associated timestamp, Flickr user identifier, title, and description (user uploaded caption). The vast majority of photos are from Manhattan, and distinct clusters can be seen around typical tourist attractions such as the World Trade Center, the Brooklyn Bridge, the Metropolitan Museum of Art, the Rockefeller Center, and up and down Broadway Avenue. The data is quite dense in certain areas. There are 6745 images with latitude beginning with 40.779 and longitude beginning with -73.963, which corresponds to 100 square meters around Metropolitan Museum of Art from 2014 only.



\begin{figure}[!htbp]
	\centering
	\begin{tabular}{cc}
		  \includegraphics[width=0.4\textwidth]{newyork_density.png} &   \includegraphics[width=0.4\textwidth]{manhattan_density.png} \\
		(a) All images & (b) Closer view of highest image density \\[6pt]
	\end{tabular}
\label{fig:densities}
\end{figure}

With 6 million images, there are roughly $1.8\times 10^{13}$ possible image pairs, which is infeasible to train a CNN on. To maximize the information learned from this corpus requires the development of smart heuristics for sampling positive and negative image pairs. The heuristics used in this paper are covered in Section \ref{sec:pair_sampling}.

Roughly 0.6\% of our image pairs have an image distance of one meter or less.

\begin{figure}[!htbp]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.4\textwidth]{2014_distance_histogram.png}  &       \includegraphics[width=0.4\textwidth]{2014_distance_5000_histogram.png}  \\
		(a) All sampled pairwise distances & (b) Within 5000 meters \\[6pt]
		\includegraphics[width=0.4\textwidth]{2014_distance_2000_histogram.png}  &       \includegraphics[width=0.4\textwidth]{2014_distance_200_histogram.png} \\
		(c) Within 2000 meters & (d) Within 200 meters\\[6pt]
	\end{tabular}
	\label{fig:distances}
\end{figure}

A simple visualization of randomly selected images at two clusters, the Metropolitan Museum of Art and the Brooklyn Bridge, reveals there is relative intracluster similarity and intercluster dissimilarity. These two sets of example clusters are extremely far apart in high-level semantic space and are perhaps not representative of the dataset as a whole, which consists of many generic street view photos. But they do demonstrate that there is significant high-level variation in image features as a function of geographic location, particularly when switching from indoor to outdoor settings. 
Though the existence of semantic clusters provides the basis for believing semantically similar pairs can be created for model training, these clusters are never explicitely identified, nor are any other similar methods used which would more strongly supervise training pair creation.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=\textwidth]{brooklynbridge.jpg}
	\caption{Photos from the Brooklyn Bridge (40.706 -73.996)}
	\label{fig:brooklynbridge}
\end{figure}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=\textwidth]{met.jpg}
	\caption{Photos from the Metropolitan Museum of Art (40.779 -73.963)}
	\label{fig:met}
\end{figure}

A non-trivial number of duplicate images, as seen in Figure \ref{fig:brooklynbridge}, indicates aggressive deduplication is required when selecting positive image pairs for training, no matter the heuristic.
 

\subsection{Pair Sampling}\label{sec:pair_sampling}
Since our full dataset is many magnitudes too large to fully use in model training as there are trillions of possible image pairs that can be created, we design sampling heuristics intended to minimize the number of pairs used while maximizing learning potential. We do this by using various distance, temporal, and other meta-data heuristics. We denote a set of pairs as $\mathcal{P}$. Unless otherwise noted, $\mathcal{P}$ will be understood to consist of similar pairs of images $(p, p^+)$ and dissimilar pairs $(p, p^-)$ in equal proportion. When referring to pairs or images, similar and positive should be considered interchangeable, as should dissimilar and negative. 

\subsubsection{Distance Heuristics}
The use of distance heuristics as a proxy for image similarity is intuitive. Images taken at the same location are more likely to be of the same object or include similar aesthetic styles than are images that are far apart. To allow for computationally tractible sampling of image pairs by distance metrics, we load sets of images into a custom implementation of a KD-Tree where images are indexed by their latitude and longitude. 

It is important to note that our latitude and longitude information is not precise. Latitude and longitude coordinates are given with six decimal places, and at the latitude of New York (roughly 40$\degree$), a 0.000001 change in latitude represents roughly one tenth of a meter, and a 0.000001 change in longitude represents roughly one thirtieth of a meter. However, though we only use images Flickr has denoted as having their highest accuracy level, Flickr's API describes this as ``street-level'' accuracy. This would imply an error bar on our image locations of about 10 meters, and therefore we cannot claim with certainty that our distance calculations follow a precise conversion from their latitude and longitude deltas. For simplicity, we take our error to be 10 meters, noting also that for the purposes of creating a weakly labeled dataset, an inability to precisely quantify this statistic is not overly concerning. This means two images with a Euclidean distance, $d$, between their coordinates can be, at worst, up to $d$ + 20 meters away. 

$\mathcal{P}_{21, 2000}$ is formed by doing a range query for all pairs $p, p^+$ within 1 meter of each other. Experimentally, the pairs that are returned have the same latitude, longitude coordinates. Taking into account the maximum geolocation error, this means $p, p^+$ are within 11 meters of each other. From these positive pairs, images $p^-$ that are farther than 2020 meters from $p$ are sampled. The distance of 2020 meters is chosen to be longer than the maximum diameter of any eyeballed cluster in Figure \ref{fig:densities}. The longest such cluster diameter is the Brooklyn Bridge at a span of 1825 meters. Thus, $\mathcal{P}_{21,2000}$ enforces a maximum distance for $p, p^+$ of 21 meters and a minimum distance for $p, p^-$ of 2000 meters. Many other distance-based datasets are created in the same fashion.

Attempts to create a dataset with negative samples $p^-$ lying at minimum $a$ and at maximum $b$ from $p$ by performing a ring query proved to be computationally intractible for the purposes of this project because of memory hashing issues. Otherwise we would have explored results for sets like $\mathcal{P}_{30,[50-100]}$, where negative samples lie farther than 50 meters but within 100 meters from the base image.

\subsubsection{Temporal Heuristics}
The Flickr corpus can be split into subsets divided by year and month. Applying distance heuristics across time divisions creates sets like $\mathcal{P}_{30,2000,June2013})$ and $\mathcal{P}_{10,2000,2013-2015})$, which refer to the distance set described intersected with the set of all possible image pairs from June 2013 and from 2013, 2014, and 2015 respectively. Datasets limited to one month are more likely to have pairs that are more similar in general, for both the positive pairs and negative pairs, whereas datasets which span years will have pairs that are less similar in general. This intuition is seen by considering that two photos taken in June of 2013 will have similar weather patterns, similar clothing worn by people, and similar ranges of daylight hours, among other factors. Two photos sampled from any time between 2013 and 2015 will be, on average, less likely to share these similarities. 

\subsubsection{Filtering by User}
As can be expected, the distance heuristic is extremely fuzzy. A selfie taken at Times Square will look very different from a family tourist photo taken at the same place, and both will look very different from a photo of a crowd waiting for the New Year. And it is highly possible that two photos taken miles apart, for example at the George Washington Bridge and the Brooklyn Bridge, or the Metropolitan Museum of Art and the Museum of Modern Art, will look fairly similar. Less fuzziness can be enforced by requiring similar pairs to have been taken by the same user. We can denote this as $\mathcal{P}_{10,2000,June2013,user})$. Dataset are also created which require both positive and negative examples to be within a certain bounding ball but differentiate positive pairs as having the same user and negative pairs as having different users. These would be denoted as $\mathcal{P}_{10,2013,user}$ if the bounding ball is 10 meters and the images are from 2013. As seen in Figure \ref{fig:users}, there are many reasons why photos taken by the same user would be more similar on average. There are similarities due to the style of the photographer (second pair), because of user interest (sporting events, third pair), and because a user has been to same places for the same reasons as himself (dinner, first pair). 
\begin{figure}[!htbp]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.4\textwidth]{user_pos.png}  &       \includegraphics[width=0.4\textwidth]{user_neg.png}  \\
							(a) Photos taken by the same users & (b) Photos taken by different users\\[6pt]
	\end{tabular}
	\caption{}
	\label{fig:users}
\end{figure}


\subsubsection{Other}
One user has uploaded thousands of photos from a time lapse of construction of the Barclays Arena in Brooklyn. These images slowly change over time as construction progresses, and they change hourly with the arrival and departure of trucks and people as well as with the lighting conditions. These images are used to construct a datset where similar images are pairs sampled entirely from this time lapse, and dissimilar images follow a minimum distance heuristic. We refer to this as $\mathcal{P}_{timelapse,2000,June2013}$. Examples of time lapse phots are shown in Figure \ref{fig:timelapse}. As can be seen, the time lapse dataset will have very similar images as its positive examples, except with lots of noise from different lighting and weather conditions and the possibility of occlusions to the baseline image in the form of passing vehicles and other transient changes to the scene. 
	
	\begin{figure}[!htbp]
		\centering
		\begin{tabular}{cc}
			\includegraphics[width=0.4\textwidth]{barclays_1.jpg}  &       \includegraphics[width=0.4\textwidth]{barclays_2.jpg}  \\
			\end{tabular}
		\caption{Two timelapse photos of the Barclays Center}
	\label{fig:timelapse}
\end{figure}
\subsubsection{Summary}
In total, 35 main datasets are created with each designed to have various degrees of label fuzziness and to require our model to focus on different features. These are summarized in Table \ref{table:datasets}. There is an incredible amount of available data, so focus is placed on generating sets from the years 2013, 2014, and 2015. For brevity, we summarize only a subset of the datasets that we ran experiments on, since many of the results will be redundant. 

\begin{table}
	\centering
	\begin{tabular}{c >{\centering\arraybackslash}m{3.2cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{2cm} c}
		\toprule
		\bfseries{Dataset} &  \bfseries Positive Heuristic & \bfseries Negative Heuristic & \bfseries Time Frame & \bfseries{Size}\\
		\midrule
		$\mathcal{P}_{21,2000,2013-2015}$ & $<$ 1m & $>$ 2000m & Jan 2013 - Dec 2015  & 176,000\\
		$\mathcal{P}_{21,2000,2015}$ & $<$ 1m & $>$ 2000m & Jan 2015 - Dec 2015  & 140,800\\
		$\mathcal{P}_{21,2013-2015,user}$ & $<$ 1m, same user & $<$ 1m & Jan 2013 - Dec 2015  & 57600\\
		$\mathcal{P}_{21,2013-2015,tl}$ & $<$ 1m, timelapse & $<$ 1m & Jan 2013 - Dec 2015  & 54400\\
		$\mathcal{P}_{10,2013-2015,user}$ & $<$ 10m, same user & $<$ 10m & Jan 2013 - Dec 2015  & 118,400\\
		$\mathcal{P}_{10,2013-2015,tl}$ & $<$ 10m, timelapse & $<$ 10m & Jan 2013 - Dec 2015  & 110,400\\
		$\mathcal{P}_{30,2013-2015,user,2h}$ & $<$ 30m, same user within 2 hours & $<$ 30m & Jan 2013 - Dec 2015  & 198,400\\
		$\mathcal{P}_{MiddleburyLR,diff\_user}$ & Left, right pairs & Different users & NA & 32000\\
		$\mathcal{P}_{MiddleburyLR,same\_user}$ & Left, right pairs& Same users  & NA& 32000\\
		$\mathcal{P}_{MiddleburyLREL,diff\_user}$ & Left, right, exposure, lighting pairs  & Different users  & NA & 32000\\
		$\mathcal{P}_{MiddleburyLREL,same\_user}$ & Left, right, exposure, lighting pairs  & Same users  & NA &32000\\
		$\mathcal{P}_{21,2000,01\_2014}$ & $<$ 1m & $>$ 2000m & Jan 2014  & 32000\\
		$\mathcal{P}_{21,2000,02\_2014}$ & $<$ 1m & $>$ 2000m & Feb 2014  & 32000\\
		$\mathcal{P}_{21,2000,03\_2014}$ & $<$ 1m & $>$ 2000m & Mar 2014 & 32000\\
		$\mathcal{P}_{21,2000,04\_2014}$ & $<$ 1m & $>$ 2000m & Apr 2014  & 32000\\
		$\mathcal{P}_{21,2000,05\_62014}$ & $<$ 1m & $>$ 2000m & May 2014  & 32000\\
		$\mathcal{P}_{21,2000,06\_2014}$ & $<$ 1m & $>$ 2000m & Jun 2014  & 32000\\
		$\mathcal{P}_{21,2000,07\_2014}$ & $<$ 1m & $>$ 2000m & Jul 2014  & 32000\\
		$\mathcal{P}_{21,2000,08\_2014}$ & $<$ 1m & $>$ 2000m & Aug 2014  & 32000\\
		$\mathcal{P}_{21,2000,09\_2014}$ & $<$ 1m & $>$ 2000m & Sep 2014  & 32000\\
		$\mathcal{P}_{21,2000,10\_2014}$ & $<$ 1m & $>$ 2000m & Oct 2014  & 32000\\
		$\mathcal{P}_{21,2000,11\_2014}$ & $<$ 1m & $>$ 2000m & Nov 2014  & 32000\\
		$\mathcal{P}_{21,2000,12\_2014}$ & $<$ 1m & $>$ 2000m & Dec 2014  & 32000\\
		$\mathcal{P}_{21,2000,01\_2015}$ & $<$ 1m & $>$ 2000m & Jan 2015  & 32000\\
		$\mathcal{P}_{21,2000,02\_2015}$ & $<$ 1m & $>$ 2000m & Feb 2015  & 32000\\
		$\mathcal{P}_{21,2000,03\_2015}$ & $<$ 1m & $>$ 2000m & Mar 2015  & 32000\\
		$\mathcal{P}_{21,2000,04\_2015}$ & $<$ 1m & $>$ 2000m & Apr 2015  & 32000\\
		$\mathcal{P}_{21,2000,05\_2015}$ & $<$ 1m & $>$ 2000m & May 2015  & 32000\\
		$\mathcal{P}_{21,2000,06\_2015}$ & $<$ 1m & $>$ 2000m & Jun 2015  & 32000\\
		$\mathcal{P}_{21,2000,07\_2015}$ & $<$ 1m & $>$ 2000m & Jul 2015  & 32000\\
		$\mathcal{P}_{21,2000,08\_2015}$ & $<$ 1m & $>$ 2000m & Aug 2015  & 32000\\
		$\mathcal{P}_{21,2000,09\_2015}$ & $<$ 1m & $>$ 2000m & Sep 2015  & 32000\\
		$\mathcal{P}_{21,2000,10\_2015}$ & $<$ 1m & $>$ 2000m & Oct 2015  & 32000\\
		$\mathcal{P}_{21,2000,11\_2015}$ & $<$ 1m & $>$ 2000m & Nov 2015  & 32000\\
		$\mathcal{P}_{21,2000,12\_2015}$ & $<$ 1m & $>$ 2000m & Dec 2015  & 32000\\
		\bottomrule
	\end{tabular}
	\caption{Datasets}
	\label{table:datasets}
\end{table}


\section{Middlebury Stereo Data}
While we can augment images to create datasets that include color, rotational, translational, and magnitude invariants, especially with the discovery of time lapse data in the Flickr set, it is more difficult to artificially construct a dataset that includes out of plane or stereoscopic invariance. In order to study the effects of this type of invariance on image similarity, we use a dataset prepared by Scharstein et al. in a study of stereo algorithms.\cite{scharstein2014high}

The dataset is very small, containing data for just 33 objects such as a motorbike, plants, and umbrella. There are four pictures per object--default left and right stereo images, a right image under different exposure, and a right image under different lighting conditions.

Examples are shown in Figure \ref{fig:middlebury}.

\begin{figure}[!htbp]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.4\textwidth]{images/Motorcycle-perfect_im0.png}  &       \includegraphics[width=0.4\textwidth]{images/Motorcycle-perfect_im1.png}  \\
		(a) Left image & (b) Right image\\[6pt]
		\includegraphics[width=0.4\textwidth]{images/Motorcycle-perfect_im1E.png}  &       \includegraphics[width=0.4\textwidth]{images/Motorcycle-perfect_im1L.png} \\
		(c) Right image, different exposure & (d) Right image, different lighting\\[6pt]
	\end{tabular}
	\label{fig:middlebury}
	\caption{Images for motorcycle}
\end{figure}

Several datasets are created with pairs from the Middlebury data taken as positive pairs, and negative pairs formed by sampling the Flickr corpus. Positive pairs are sampled either entirely from the set of left, right stereo images associated with each object, or they are sampled by drawing pairs from the four left, right, exposure, and lighting images associated with each object. For negative pairs, images are sampled from Flickr that are within 10 meters of each other and either taken by the same or different users.

\begin{table}
	\centering
	\begin{tabular}{c >{\centering\arraybackslash}m{3.2cm} >{\centering\arraybackslash}m{3cm} c}
		\toprule
		\bfseries{Dataset} &  \bfseries Positive Heuristic & \bfseries Negative Heuristic & \bfseries{Size}\\
		\midrule
		$\mathcal{P}_{MiddleburyLR,diff\_user}$ & Left, right pairs & Different users  & 32000\\
		$\mathcal{P}_{MiddleburyLR,same\_user}$ & Left, right pairs& Same users  & 32000\\
		$\mathcal{P}_{MiddleburyLREL,diff\_user}$ & Left, right, exposure, lighting pairs  & Different users  & 32000\\
		$\mathcal{P}_{MiddleburyLREL,same\_user}$ & Left, right, exposure, lighting pairs  & Same users  & 32000\\
		\bottomrule
	\end{tabular}
	\caption{Middlebury data}
	\label{table:middlebury}
\end{table}


\section{Google Image Data}
A manually labeled dataset is still needed for testing. We use a dataset published by Wang et al., which will be referred to as the Wang set.\cite{wang2014learning}. 
The Wang set consists of 5033 image triplets. The dataset was curated by sampling triplets of images, $(Q, A, B)$ from the top 50 search results for 1000 popular text queries using the Google image search engine. Most text queries are thus represented multiple times. Human raters were given four choices in ranking the similarity of images in the triplets: 1) both $A$ and $B$ were similar to $Q$; 2) both $A$ and $B$ were dissimilar to $Q$; 3) $A$ was more similar to $Q$ than $B$; 4) $B$ was more similar to $Q$ than $A$. Each triplet was rated by three different humans. If all three ratings were the same, the triplet was included in the dataset.

The Wang set contains an extremely wide variety of images due to its creation through sampling popular Google image searches. A random sampling of the image categories returns \texttt{\justify Lynda Carter, Paris skyline, Empire State building, brunette, Bob Marley, Angora Rabbit, Jeep Liberty, 2 Fast 2 Furious, Shemar Moore, soccer ball, motorbike racing, Brittany Murphy}. A plurality of classes refer to people, mostly celebrities.

Table \ref{table:random_triplets} displays a random sampling of triplets. In contrast to the Flickr data, for which a non-trivial proportion of triplets $(p, p^+, p^-)$, will have all three images be relatively dissimilar, the Wang set has a high proportion of triplets where all three images are extremely similar. Despite the requirement of unanimous agreement by the three human raters in the creation of this dataset, there are some examples we feel may be mislabeled or should not have been included in the dataset. In Table \ref{table:hard_triplets}, we show a few examples demonstrating the relatively narrow margin between similar and dissimilar pairs, and in Table \ref{table:variant_triplets} a few examples demonstrating how image variants appear to have played a role in the labeling of the Wang set.

\begin{table}
	\begin{tabular}{>{\centering\arraybackslash}m{1in} >{\centering\arraybackslash}m{1.4in} >{\centering\arraybackslash}m{1.4in} >{\centering\arraybackslash}m{1.4in}}
		\toprule
		\bfseries Image Query & \bfseries Base Image ($p$) & \bfseries Similar Image ($p^+$) & \bfseries Dissimilar Image ($p^-$) \\
		\midrule
		\centering New York City & \includegraphics[width=1.4in]{images/nyc_base.jpeg} & \includegraphics[width=1.4in]{images/nyc_pos.jpeg} & \includegraphics[width=1.4in]{images/nyc_neg.jpeg}\\
		Bart Simpson & \includegraphics[width=1.4in]{images/bart.jpeg} & \includegraphics[width=1.4in]{images/bart_pos.jpeg} & \includegraphics[width=1.4in]{images/bart_neg.jpeg}\\
		Sonic boom & \includegraphics[width=1.4in]{images/sonic_boom.jpeg} & \includegraphics[width=1.4in]{images/sonic_boom_pos.jpeg} & \includegraphics[width=1.4in]{images/sonic_boom_neg.jpeg}\\
		Column1d & Column2d & Column3d & \\
		\bottomrule
	\end{tabular}
	\caption{Random triplets from Wang Set}
	\label{table:random_triplets}
\end{table}

\begin{table}
	\begin{tabular}{>{\centering\arraybackslash}m{1in} >{\centering\arraybackslash}m{1.4in} >{\centering\arraybackslash}m{1.4in} >{\centering\arraybackslash}m{1.4in}}
		\toprule
		\bfseries Image Query & \bfseries Base Image ($p$) & \bfseries Similar Image ($p^+$) & \bfseries Dissimilar Image ($p^-$) \\
		\midrule
		\centering Monument Valley & \includegraphics[width=1.4in]{images/monument_valley_base.jpeg} & \includegraphics[width=1.4in]{images/monument_valley_pos.jpeg} & \includegraphics[width=1.4in]{images/monument_valley_neg.jpeg}\\
		Guitar & \includegraphics[width=1.4in]{images/guitar.jpeg} & \includegraphics[width=1.4in]{images/guitar_pos.jpeg} & \includegraphics[width=1.4in]{images/guitar_neg.jpeg}\\
		Nirvana & \includegraphics[width=1.4in]{images/nirvana.jpeg} & \includegraphics[width=1.4in]{images/nirvana_pos.jpeg} & \includegraphics[width=1.4in]{images/nirvana_neg.jpeg}\\
		\bottomrule
	\end{tabular}
	\caption{Hard triplets from Wang Set}
	\label{table:hard_triplets}
\end{table}

Though Wang et al. took steps to ensure a fairly clean dataset by requiring unanimous rankings by three different rankers, we feel the dataset is not particularly clean and in fact contains a non-trivial number of ambiguous similarity rankings. As show in Table \ref{table:hard_triplets}, the watermarked Monument Valley image seems less similar, as does the pink guitar, because of the presence of a magazine. There are many examples similar to the Nirvana triplet where the differences between $p, p^+,$ and $p^-$ are incredibly subtle. We do not remove these examples to "clean" the dataset, but mention them only to note that though the Wang set was created through three unanimous decisions, that does not mean we should expect it to be entirely pure.

This is not the say that the dataset is malformed. There are many instances of easily separable rankings, such as the one shown in Table \ref{table:easy_triplet}.
\begin{table}
	\begin{tabular}{>{\centering\arraybackslash}m{1in} >{\centering\arraybackslash}m{1.4in} >{\centering\arraybackslash}m{1.4in} >{\centering\arraybackslash}m{1.4in}}
		\toprule
		\bfseries Image Query & \bfseries Base Image ($p$) & \bfseries Similar Image ($p^+$) & \bfseries Dissimilar Image ($p^-$) \\
		\midrule
		\centering Parthenon & \includegraphics[width=1.4in]{images/parthenon.jpeg} & \includegraphics[width=1.4in]{images/parthenon_pos.jpeg} & \includegraphics[width=1.4in]{images/parthenon_neg.jpeg}\\
		\bottomrule
	\end{tabular}
	\caption{An easy triplet from the Wang Set}
	\label{table:easy_triplet}
\end{table}


\begin{table}
	\begin{tabular}{>{\centering\arraybackslash}m{1in} >{\centering\arraybackslash}m{1.4in} >{\centering\arraybackslash}m{1.4in} >{\centering\arraybackslash}m{1.4in}}
		\toprule
		\bfseries Image Query & \bfseries Base Image ($p$) & \bfseries Similar Image ($p^+$) & \bfseries Dissimilar Image ($p^-$) \\
		\midrule
		\centering New York City & \includegraphics[width=1.4in]{images/freedom_tower.jpeg} & \includegraphics[width=1.4in]{images/freedom_tower_pos.jpeg} & \includegraphics[width=1.4in]{images/freedom_tower_neg.jpeg}\\
		Sydney Opera House & \includegraphics[width=1.4in]{images/sydney_opera_house_base.jpeg} & \includegraphics[width=1.4in]{images/sydney_opera_house_pos.jpeg} & \includegraphics[width=1.4in]{images/sydney_opera_house_neg.jpeg}\\
		Michelangelo & \includegraphics[width=1.4in]{images/michelangelo.jpeg} & \includegraphics[width=1.4in]{images/michelangelo_pos.jpeg} & \includegraphics[width=1.4in]{images/michelangelo_neg.jpeg}\\
		Picasso & \includegraphics[width=1.4in]{images/picasso.jpeg} & \includegraphics[width=1.4in]{images/picasso_pos.jpeg} & \includegraphics[width=1.4in]{images/picasso_neg.jpeg}\\
		\bottomrule
	\end{tabular}
	\caption{Variant influenced triplets from Wang Set}
	\label{table:variant_triplets}
\end{table}

Table \ref{table:variant_triplets} shows that human rankers quite often rely on image variants in their judgements of similarity. For ``New York City'', zoom variance is important; for the ``Sydney Opera House'' and ``Michelangelo'', illumination and out-of-plane rotation variance comes into play; and for ``Picasso'', the rankers seem to have picked up on some similarity in hue.


\subsection{Quantifying the Amount of Fuzziness}
One way of quantifying the fuzziness of the sampled Flickr dataset labels is by extracting image embeddings for each image using a model trained on ImageNet andcompute the mean and median distances for positive pairs and negative pairs. Another useful statistic is the percentage of triplets with ranking violations, where the positive distance is greater than the negative one. Figure \ref{fig:pos_neg_distances} shows that the distribution of distances for positive and negatives pairs is highly similar, with very little separation between means of the two distributions. In fact, $\mathcal{P}_{20,2013-2015,user}$, has an inversion rate of 52\%, mean that dissimilar pairs are actually closer on average in the ImageNet embedding space. The average inversion rate for all datasets is 36.6\%, which is much higher than the rate of out-of-class errors of 16\% in Krause et al.'s study.\cite{krause2016unreasonable} A full table of distances and inversion is listed in Appendix \ref{section:a.data}. Not including the time lapse or Middlebury datasets, the three lowest rates of inversion are 0.1034, 0.2098 and 0.3022, and the three highest are 0.5257, 0.4481 and 0.4032.

\begin{figure}[!htbp]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.4\textwidth]{distances/2013-5_1m_user.png}  &       
		\includegraphics[width=0.4\textwidth]{distances/2013-5_10m_user.png}  \\
		(a) $\mathcal{P}_{21,2013-2015,user}$ & (b) $\mathcal{P}_{30,2013-2015,user}$\\[6pt]
		\includegraphics[width=0.4\textwidth]{distances/2013-5_30m_user_2h.png}  &       \includegraphics[width=0.4\textwidth]{distances/new_2013_2014_2015_all.png} \\
		(c) $\mathcal{P}_{30,2013-2015,tl,2h}$ & (d) $\mathcal{P}_{21,2013-2015}$\\[6pt]
	\end{tabular}
	\label{fig:pos_neg_distances}
	\caption{Positive and negative pair distances}
\end{figure}


\section{Differences with ImageNet}
ImageNet is a dataset built on top of WordNet, which is a graph of words and phrases with similar semantic concepts connected to each other. There are 100,000 synonym sets, which are strongly connected local clusters, in WordNet. ImageNet aims to provide 1000 example images for each synonym set, with all images quality-controlled and human annotated. Notably, ImageNet currently only provides examples for nouns, meaning each image has an object as its central focus. With at least 1000 example images per object, it is impossible for a deep CNN trained on ImageNet not to learn certain types of image invariants. 

In contrast, the Flickr set contains no class segmentations, so it is unlikely for the same types of generalized image invariants to be learned. A upside-down image, which might force an ImageNet model to learn rotational invariance, will not be tied to a label in the Flickr set. The vast diversity of the Flickr images also contributes to its effectiveness in learning the importance of variants that ImageNet might not prioritize. It is likely that if human annotators labeled all the Flickr photos from New York, there would be at least a magnitude more classes than in ImageNet. Figure \ref{fig:example_flickr} some randomly selected Flickr images which display the diversity of image space the data spans, as well as a lack of quality-control, meaning many images will not be focused on a particular object or scene to the degree ImageNet images are.

\begin{figure}[!htbp]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.4\textwidth]{r_1.jpg}  &       
		\includegraphics[width=0.4\textwidth]{r_3.jpg}  \\
		\includegraphics[width=0.4\textwidth]{r_4.jpg}  &       
		\includegraphics[width=0.4\textwidth]{r_2.jpg} \\

	\end{tabular}
	\label{fig:example_flickr}
	\caption{Random Flickr images}
\end{figure}





