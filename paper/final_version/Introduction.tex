
\section{Image Similarity}
What does it mean for two images to be similar? Are two red-hued images similar, even if they are of different objects? Are an image of a cardinal and an image of a blue jay similar because they both show birds? Regardless of the semantic interpretation of the word 'similar', any model that determines whether or not two images are similar must first be able to embed them in a visual representation.

Learning visual representations for images has been important for a variety of tasks, including image classification, semantic segmentation, object detection, and even geolocating images. The methods of extracting image representations have changed greatly over the years, moving from manually defined features like histograms of oriented gradients (HOGs) and scale-invariant feature transforms (SIFT)\cite{lowe1999object}\cite{dalal2005histograms}, to the current state of the art of extracting feature vectors from convolutional neural networks, beginning with the seminal work of Krizhevsky et al. which achieved classification results for the 2012 Imagenet ILSVRC image classification contest far surpassing previous state-of-the-art benchmarks\cite{krizhevsky2012imagenet}. Yet despite the change in techniques, the basic requirements for model training have remained the same. First, large quantities of reliably labeled data must be gathered, and then a model, usually a CNN, can trained on this data. With the popularization of CNNs, often Mechanical Turkers have employed to create massive and highly pure labeled datasets.

The popularization of large scale image datasets such as ImageNet, which contains 14,197,122 images belonging to 1000 classes\cite{deng2009imagenet}, the MIT Places dataset which contains 7 million images for scene classification\cite{zhou2014learning}, the SUN scene classification database\cite{xiao2010sun}, and the Microsoft COCO dataset of 2.5 million images for common objects in context\cite{lin2014microsoft} have allowed researchers to build models highly adept at basic image classification tasks\cite{russakovsky2013detecting}. In a review of deep learning models trained on these massive datasets with many basic image classes (ImageNet contains classes for many animals and plants and items such as 'tennis ball', 'fountain pen', and 'tricycle') Russakovsky et al. concluded that deep learning techniques were able to transfer learning from these dataset classes to other generic classes, such as distinguishing dogs from airplanes.\cite{russakovsky2013detecting} However, more fine-grained image classification, such as between species of flowers, or of dogs of different ages, required learning different image embeddings.

After the deep learning revolution, a major direction of image understanding research has been to develop more fine-grained datasets for the training of models for highly specific tasks. A quick search for image datasets published in 2016 returns ones for irises, ultrasounds, weather property, tumors, light fields, and food calories.\cite{bowyer2016nd}\cite{cortes2016ultrasound}\cite{chu2016image2weather}\cite{shi2016stacked}\cite{paudyal2016smart}\cite{pouladzadeh2015foodd}. Yet, although we have been able to train more and more specific models for finer and finer grained image classification, this research still relies on the gathering of accurately labeled data. It is infeasible to gather large quantities of data for every possible image understanding task.

Fine-grained similarity also pushes against a fundamental tenant of machine vision: the desire for algorithms to capture various types of invariance--rotation, translation, scaling, illumination, color, etc. The importance of invariance to the field of machine vision is enshrined in the seminal technique of using scale-invariant feature transformations. Krishevsky et al. note also that their scheme "approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color fo the illumination."\cite{krizhevsky2012imagenet} Standard augmentation techniques in deep learning include flipping images horizontally and vertically, rotation images, extracting random crops and zoom, and jittering color and intensity channels with random noise, all with the purpose of forcing models to learn these types of invariants. These techniques make sense for many types of contemporary vision-related tasks such as object recognition, bounding box detecting, and scene classification, since a red car and blue car are still both cars, and the presence of either might inform an algorithm that a scene depicts a highway. Yet, for fine-grained image similarity purposes where we are attempting to determine which two images are most similar out of a red car, blue car, and red car, we do not want our model to learn color invariance. Similar considerations for rotational, translational, and other types of invariance must also be made.

Thus, the challenge in the task attempted is evident: we seek to 1) learn to detect image structures useful for embedding them in a similarity space, 2) learn to do so using a weakly labeled dataset, and 3) learn to distinguish images on a very fine-grained level, which may require learning an embedding that ignores invariants that might make task 1) easier.

\section{Weakly Supervised Training}
TODO: moving toward the Holy Grail of unsupervised training

Some more recent approaches have looked into augmenting highly supervised training with weakly supervised web data, thus greatly increasing the amount of data available for these highly specific image understanding tasks. Xu et al.\cite{xu2015augmenting} use existing datasets to learn feature representations and part-based object classifiers. They then extract accurate part labels from fuzzy web image data. Kraus et al take this line of work even further and use generic image recognition techniques on noisy web data and exceed state-of-the-art classification accuracies on the CUB-200-2011 dataset, without using any manually labeled data.\cite{krause2016unreasonable}

\section{Our Approach}
This paper follows this recent work in utilizing the large quantity of image data available online through search engines and image hosting sites like Google and Flickr. Rather than use web queries to form fuzzy classes for image classification like Kraus et al, we seek to use geo-tagged images uploaded to Flickr to learn an image embedding useful for image similarity tasks. We do not train on any image pairs manually labeled as similar but rather rely on the physical geographic distance between two images to inform the training of our model. We explore how well geographic distance can serve as a stand-in for manually labeled similarity data, with a particular focus on exploring heuristics for sampling pairs of similar images for maximal learning efficiency.

We choose to use deep learning representations of images trained through a Siamese network rather than use manually crafted features such as Gabor filters, scale-invariant feature transforms (SIFT), or histograms of oriented gradients (HOG), believing that allowing a model to learn features on its own will be more robust. In a departure from the research of Xu et al and Kraus et al, we also work with a classless representation of our data. We do not assign an image to a fuzzy class, such as belonging to the category 'bridge', but associate it only with its latitude and longitude data. In a sense, this means we treat each image as belonging to a unique geolocation class of size 1.

Using our developed heuristics for sampling pairs of similar images for comparison with dissimilar images, we train a deep Siamese network to learn a low dimensional feature representation, with an objective of learning that pairs of images close in physical distance should be closer in our image embedding space.

The contributions of this paper are [to be completed]
