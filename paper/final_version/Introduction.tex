
\section{Image Similarity}
What does it mean for two images to be similar? Can two images be similar if they are focused on different types of objects? Which two teapots in Figure \ref{fig:teapots} are the most similar?

\begin{figure}[!htbp]
	\centering
	\begin{tabular}{ccc}
		\includegraphics[width=0.3\textwidth]{utah_teapot.jpg} &  
		\includegraphics[width=0.3\textwidth]{teapot_2.jpg} & \includegraphics[width=0.3\textwidth]{teapot_3.jpeg} \\
		(a) A teapot & (b) A teapot & (c) A teapot\\[6pt]
	\end{tabular}
	\label{fig:teapots}
	\caption{Three teapots}
\end{figure}

Here we caption all three teapots with the same innocuous description, which hides the difficulties in categorizing objects and, by extension, images. Perhaps more descriptive captions would be \texttt{(a) A white teapot, (b) A Japanese teapot, (c) A teapot in a restaurant}. Even more verbose descriptions might note that the white teapot appears to be a digital rendering, that the middle teapot is lit as if in a museum display, and that the third teapot is partially out of frame and not the only object of importance in the image. Esteemed computer science faculty or even just middling computer science undergraduates might exclaim that the first teapot is the most remarkable of the three. The Utah teapot, as it is known, is perhaps the most famous teapot in the world due to its use as a stock computer graphics model.

We see that the task of understanding images is complex. There are an infinite number of facets through which we can describe any given image. Some of these can be quantified: presence of teapot, color, hue, contrast, main object, number of foreground/background divisions, lighting, and so on. Other qualities are far more nebulous. \texttt{Famous} as a descriptor for the first image seems questionable--even if we agreed it was "famous," how would one quantify fame? Yet clearly there is a strong hidden attribute to the Utah teapot, because when considering the image of Lenna Soderberg in Figure \ref{fig:lenna}, another famous image from computer graphics, some readers of this paper would agree that Lenna is in fact more similar to the Utah teapot than either of the actual teapots.

\begin{figure}[!htbp]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.3\textwidth]{utah_teapot.jpg} &
		\includegraphics[width=0.3\textwidth]{lenna.png}  \\
		Utah teapot & Lenna Soderberg\\[6pt]
	\end{tabular}
	\label{fig:lenna}
	\caption{A similar pair}
\end{figure}

In the same vein, an image of the Empire State Building might be considered more similar to the Eiffel Tower than to a standard apartment complex in Brooklyn. Two red teapots might be more similar to each other than to a green teapot, unless one of the red teapots has the same type of long spout as the green one and the other doesn't.

In the past, the field of computer vision has considered images to be similar so long as both images focused on the same type of object. Gradually, our definition of what constitutes the same object has narrowed. Figure \ref{fig:old_pairs} shows two flowers of different species and two differently posed girls that were matched as similar in a 1998 study on perceptual similarity.\cite{rogowitz1998perceptual} With some revolutionary new algorithms, it has started to become possible to train models to detect fine-grained similarity, which differentiates objects and images not only on their coarse physical characteristics but also on highly complex combinations of these characteristics, as well as soft concepts such as "Fame."

\begin{figure}[!htbp]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.3\textwidth]{girls.png} &
		\includegraphics[width=0.3\textwidth]{flowers.png}  \\
		Similar girls & Similar flowers \\[6pt]
	\end{tabular}
	\label{fig:old_pairs}
	\caption{Similar pairs from 1998}
\end{figure}

Learning to differentiate images on such a scale has seen use in wide-ranging applications, such as in scientific research in classifying different species, of insects and birds; in facial recognition software; in helping consumers purchase products according to aesthetic preferences; and in building search-by-example databases.\cite{krause2016unreasonable}\cite{chopra2005learning}\cite{bell2015learning}\cite{wang2014learning}

The task of detecting image similarity boils down to learning how to represent images. If we decide two teapots are similar if they are both red, we are choosing to represent our images with a field that denotes the presence of a certain color. Once we have a complex enough representation for an image, a formulation for similarity is easily computed as the distance between the two images in their embedded representations.

Learning image representations is important for almost every task in machine vision, including image classification, understanding scenes, semantic segmentation, object recognition, answering questions about images, captioning videos, and even geolocating images. The methods of extracting image representations have changed greatly over the years, moving from using manually defined features like histograms of oriented gradients (HOGs)\cite{lowe1999object} and scale-invariant feature transforms (SIFT)\cite{dalal2005histograms}, to the current state-of-the-art technique of extracting feature vectors from convolutional neural networks, beginning with the seminal work of Krizhevsky et al.\cite{krizhevsky2012imagenet}. We will discuss these techniques and the ways in which image representations have changed in more depth in Chapter \ref{chapter:background}. In our view, there are two key negative characteristics associated with deep learning techniques as applied to machine vision: 1) Most deep learning is strongly supervised, which means that techniques are only as good as the magnitude or quality of their data; and 2) The ability of deep learning techniques to learn and enforce image invariants with incredible effectiveness has masked a detrimental shift of attention away from some features which are important to human-level perception.

\section{Data Constraints}
As methods for extracting image representations have shifted from manual definition to the more black box art of training deep convolutional networks, our reliance on large quantities of training data has increased. Though deep learning requires relatively little prior knowledge about a classification task in contrast with classical vision techniques, which require domain knowledge, or Bayesian techniques, which build generative models with carefully tuned priors, deep learning techniques can still require tens of thousands or millions of images per class. The flexibility that deep learning advocates tout in comparison to classical and Bayesian techniques really only disguises an extreme inflexibility of a different form. In initial years, deep learning's need for vast amounts of data was satisified by the popularization of large scale image datasets such as ImageNet, which contains 14,197,122 images belonging to 1000 classes\cite{deng2009imagenet}; the MIT Places dataset, which contains 7 million images for scene classification\cite{zhou2014learning}; the SUN scene classification database\cite{xiao2010sun}; and the Microsoft COCO dataset of 2.5 million images for common objects in context\cite{lin2014microsoft}. These and other datasets have allowed researchers to build models that are highly adept at basic image classification tasks\cite{russakovsky2013detecting}. In a review of deep learning models trained on these massive datasets with many basic image classes (ImageNet contains classes for many animals, plants, and basic items such as 'tennis ball', 'fountain pen', and 'tricycle'),  Russakovsky et al. concluded that deep learning techniques were able to transfer learning from these dataset classes to other generic classes, e.g., distinguishing dogs from wolves.\cite{russakovsky2013detecting} However, as the field has matured and begun turning its focus to more challenging tasks, such as fine-grained similarity rather than just similarity, it has become apparent that more expansive datasets, or at least more robustly labeled datasets, are needed.

These datasets are often built by hiring mechanical Turkers, who are given instructions on how to label or annotate millions of images. A quick search for image datasets published in 2016 returns ones for irises, ultrasounds, weather property, tumors, light fields, and food calories.\cite{bowyer2016nd}\cite{cortes2016ultrasound}\cite{chu2016image2weather}\cite{shi2016stacked}\cite{paudyal2016smart}\cite{pouladzadeh2015foodd} Yet although we have been able to train more and more specific models for these specific types of image classification, this research still relies on the gathering of accurately labeled data. It is infeasible, both from manpower and expense standpoints, to gather large quantities of data for every possible image understanding task.


\subsection{Alternatives to Strongly Supervised Learning}
The field of machine learning can be roughly segmented into three types of categories:
\begin{enumerate}
	\item Supervised learning: models are trained on example inputs and desired outputs, with the desired outputs known and labeled with a high degree of correctness
	\item Unsupervised learning: models are trained without knowing the desired outputs; models are intended to detect structures or patterns in the input data without guidance
	\item Reinforcement learning: models learn to interact with an environment with possible rewards and punishments
\end{enumerate}

Semi-supervised learning, which provides partially labeled data for the model, and weakly supervised learning, which provides possibly incorrectly labeled data to the model, are other forms of learning. While reinforcement learning has limited applicability to agent-less computer vision problems like similarity, strong interest in unsupervised, semi-supervised, and weakly supervised learning has arisen in the field, both in response to the recognition that large-scale and fine-grained strong supervision is infeasible, and because of general theoretical interest. Rather than label tens of millions of images, semi-supervised learning might provide several tens of thousands of labeled images and attempt to self-label unlabeled images to increase the amount of available training data. Weakly supervised learning accepts that there will be some non-trivial level of error in the labeled data. Many researchers have begun using weakly labeled but publicly available, and thus vast, image data, such as images mined through Google image searches, to train models.

Such techniques can be surprisingly powerful. In training a classifier on 14,000 different classes, Kraus et al. generate a training set by pulling Google image data and exceed state-of-the-art classification accuracies on the CUB-200-2011 dataset, despite the fact that images returned by their image searches had 16\% out-of-class error.\cite{krause2016unreasonable} One type of weakly supervised learning, called distantly supervised learning, relies on labeling datasets using some heuristic. Xu et al.\cite{xu2015augmenting} use existing datasets to learn feature representations and part-based object classifiers. They then extract accurate part labels from fuzzy web image data. If the labeling heuristic used is accurate enough, this kind of weakly supervised learning can be nearly as good as strongly supervised techniques. The possible advantages of non-supervised learning are obvious--Kraus et al.'s landmark work is notable not only because it exceeded state-of-the-art results with weak supervision, but also because they were able to scale their classifier to detect 14,000 different classes by expending only extra machine hours, and no man hours.


\subsection{Image Invariants}
The machine vision field's reliance on deep learning has also caused learning invariants to become an almost unquestioned tenant for building a vision model. Krizhevsky et al.'s 2012 submission to the ILSVRC notes that their use of data augmentation, which has become standard in training deep convolutional neural nets, "approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination."\cite{krizhevsky2012imagenet} Even before Krizhevsky, libraries for HOGs supported flip invariants, and the popular scale-invariant feature transformation technique, pioneered in 1999, obviously prioritizes scale-invariance.\cite{vedaldi2010vlfeat}\cite{muja2014scalable} 

The proliferation of deep CNNs has been matched by work in augmenting data to learn these and other types of invariance. Cropping and zooming images, applying rotations, shears, translations, flips, and out-of-plane rotations are all common augmenting techniques used to learn the associated invariance. It seems that even without strong data augmentation techniques, the architecture of deep CNNs naturally lend themselves to learning image invariants. In a study on image invariants, Lenc and Vidaldi find that common invariants present in natural samplings of object classes, such as horizontal flips and rescaling transformations, will be learned by the third or fourth convolutional block in a deep network. The presence of these invariants has to do with the construction of the ImageNet dataset, which has millions of images for just 1000 classes, thus ensuring that a wide variety of each type of class is represented, with each example having its own lighting, rotation, scale, etc, and with all examples labeled as identical. Lenc and Vidaldi conclude that the representation of deep CNNs transform in predictable ways in response to image augmentations, and that these representations are largely interchangeable among different architectures, at least for the invariants learned in the shallower layers of a network. Deeper layers, which learn task-specific filters, are more specific to their architecture.\cite{lenc2015understanding} The learning of these invariants has been proven time and time again to be extremely useful across a spectrum of vision tasks, and this is intuitive. Our three teapots are all teapots despite differences in color or lighting conditions. Empirically, the learning of intensity and color invariance improved Krizhevsky et al.'s top-1 error rate by over 1\%.\cite{krizhevsky2012imagenet} 

Yet fine-grained similarity pushes a little against this tenant of machine vision. On a coarse level, invariants are useful for recognizing that two teapots are more similar to each other than they are to a dog, despite changes in rotation, lighting, color, etc. But when comparing three teapots, image variants must come back into play. We often compare vision models with human-level performance, and when it comes to fine-grained similarity, combinations of color and lighting and other traditionally ignored variants play important roles in our estimations of aesthetic quality. A good image similarity model thus must not only be able to detect the physical characteristics of objects, but must also be able to determine semantic information about the image. 


\section{Motivations}
The direction and motivation for our research comes directly from considering these two key characteristics of deep vision models. We seek to
\begin{enumerate}
	\item Learn an embedding function useful for comparing the similarity of images on a very fine-grained level
	\item Learn this embedding using a weakly labeled and publicly available dataset
	\item Prove that our results are generalizable to tasks beyond those that would normally be serviced by our dataset
\end{enumerate}

Along the way, we investigate the importance of certain invariants in our learned similarity model and attempt to quantify the trade-offs in extra training time required for training with a weakly labeled dataset.

We use geo-tagged images uploaded to Flickr to learn our embedding by labeling pairs of images as similar or dissimilar using heuristics derived from the meta-data associated with each image. We explore how well these heuristics can serve as a stand-in for manually labeled similarity data, with a particular focus on exploring heuristics for sampling pairs of similar images for maximal learning efficiency. As discussed, while we recognize the importance of detecting image invariants, we also attempt to train a model that can judge whether image variants should be more strongly considered in computing image similarity for certain examples than they would be by a more traditional deep architecture. To this end, we train a multi-scale and multi-module Siamese network with a pairwise loss function, with the objective of learning that pairs of images more likely to be considered similar by humans should be closer together in our image embedding space than are pairs likely to be considered relatively dissimilar. One module, which is relatively shallow, is trained to preserve the presence of variants in our image embedding, and the other, which is of standard depth, is trained to enforce the presence of invariants in the embedding.

\section{Outline}
This paper is organized as follows. In Chapter \ref{chapter:background} we present a more in-depth discussion about the background of vision research, especially as pertaining to the techniques used to generate image embeddings and relating to image similarity. In Chapter \ref{chapter:data}, we discuss our publicly available dataset, the Flickr set, as well as a dataset used for evaluation and a dataset used for additional exploration into image similarity. In Chapter \ref{chapter:network}, we present our network formulations and training techniques. We report and discuss our experimental results in Chapter \ref{chapter:experiments}. We conclude with a summary and an outline of potential future work in Chapter \ref{chapter:conclusion}.

