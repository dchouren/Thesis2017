We have three main experimental goals:
\begin{enumerate}
	\item Prove it is possible to learn a useful image embedding function from publicly available and fuzzily labeled data
	\item Investigate, and quantify if possible, the effects of data fuzziness on model learning rates and generalizability
	\item Investigate how the presence of certain invariants in our data affects model generalizability
\end{enumerate}




\section{Weakly Supervised Image Embedding}
Our main experiment, to find a network formulation which can learn a useful image embedding from fuzzily labeled data, is represented by the pipeline shown in Figure \ref{fig:variant_model_train}. As a recap, we compare two different methods used to train the networks that compose $C$: 1) concatenate the vector output from a frozen $I$ with the vector output from $V$, and update $V$ only; and 2) take only the vector output from $V$ and update $V$. We designed $V$ to arbitrarily produce an output vector of size 1024. We use an unmodified ResNet50 model pretrained on the ImageNet dataset as $I$, which has an output vector of size 2048. We will refer to the training of these methods as $T_c^1$ and  $T_c^2$. We refer to the training of the blending network as $T_b$. We summarize some basic statistics about these training regimes in Table \ref{table:embedding_parameters}. We experiment with different layer widths for the small blending model, but on the whole, the number of training parameters is magnitudes smaller than the training of the $C$ units.

Training is done on either a Tesla K20m GPU with 5GB of memory or on a Tesla P100 GPU with 16GB of memory. Do to memory constraints, any models run on the K20m GPU are trained with batch sizes of 6. Models trained on the P100 GPU are trained with batch sizes of 32. All training is single GPU. 

\begin{table}
	\begin{tabular}{*{4}{c}}
		\toprule
		\bfseries Training Method & \bfseries Units Trained & \bfseries Trainable Parameters & \bfseries Output Size\\
		\midrule
		$T_c^1$ & $V$ ($I$ frozen) & 47,822,464 & 3072\\
		$T_c^2$ & $V$ & 34,587,776 & 1024\\
		$T_b$ & $B$ ($V$ and $I$ frozen) & 393,216 to 12,582,912 & 128 to 1024\\
		\bottomrule
	\end{tabular}
	\caption{Training statistics}
	\label{table:embedding_parameters}
\end{table}

\subsection{Optimizer Experiments}
We considered several optimizers: stochastic gradient descent (SGD), RMSprop\cite{tieleman2012lecture}, Adagrad\cite{duchi2011adaptive}, Adadelta\cite{zeiler2012adadelta}, Adam\cite{kingma2014adam}, and Nadam\cite{kingma2014adam}. Before beginning training, we run a small subsampling of our data to select our optimizer based on validation loss. While Nadam proved to be the most volatile, often causing exploding gradient updates, it also proved the best at finding successively deeper local minima. As we show in Figure \ref{fig:nonsmooth training}, the loss space of our problem is highly rough. Nadam is Adam with Nesterov momentum. The Adam optimizer essentially combines momentum, which averages the direction of gradient updates with an exponential decay parameter to find the proper update vector direction, with RMSprop, which determines the direction of the update vector. Nadam is an adaptive-learning method, meaning that a learning rate schedule is not necessary. We use standard hyperparameters for Nadam, with an initial learning rate of 0.002, $\beta_1=0.9$, $\beta_2=0.999$, and $\epsilon=10^{-8}$.

\subsection{Training Module $C$}
Despite the extreme fuzziness of our labels as shown in Table \ref{table:data_fuzziness}, we find that our choices of architecture, optimizer, and loss function are able to quickly locate local minima. Validation loss plummets without delay, often reaching inflection points in the loss curve within three training epochs. Learning typically has enough momentum to escape local minima and find deeper minima.

As our data is weakly labeled, it is impossible to quantify model performance with any sort of validation or test accuracy. Instead we must use the Wang dataset for a hard accuracy quantification. We present validation losses in Table \ref{table:val_loss}. Because these validation losses are for different datasets, however, we do not use the normal approach of selecting a training method with the lowest validation loss and computing its test accuracy. Instead, we use each trained model in an ensemble prediction. Note that though we have mentioned many datasets, some of these like the Middlebury datasets, are trained for exploration of image invariants, and some are for exploring the effect of an increase in the number of data samples on training. Our final ensemble will consist of the models trained on $\mathcal{P}_{1,2013-2015,user}, \mathcal{P}_{1,2000,2015}, \mathcal{P}_{1,2000,2013-2015}, \mathcal{P}_{10,2013-2015,user}$, and the monthly datasets from 2014, $\mathcal{P}_{1,2000,01\_2014}$ through $\mathcal{P}_{12,2000,01\_2014}$.  We do have 

\begin{table}
	\centering
	\begin{tabular}{c >{\centering\arraybackslash}m{3.2cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{2cm} >{\centering\arraybackslash}m{2cm} c}
		\toprule
		\bfseries{Dataset} &  \bfseries Epochs & \bfseries Samples & \bfseries $T_c^1$ Validation Loss & \bfseries $T_c^2$ Validation Loss & Ranking Violations\\
		\midrule
		$\mathcal{P}_{1,2000,2013-2015}$ & $<$ 1m & $>$ 2000m & Jan 2013 - Dec 2015  & 176000\\
		$\mathcal{P}_{1,2000,2015}$ & $<$ 1m & $>$ 2000m & Jan 2015 - Dec 2015  & 140800\\
		$\mathcal{P}_{1,2013-2015,user}$ & $<$ 1m, same user & $<$ 1m & Jan 2013 - Dec 2015  & 57600\\
		$\mathcal{P}_{1,2013-2015,tl}$ & $<$ 1m & $<$ 1m, timelapse & Jan 2013 - Dec 2015  & 54400\\
		$\mathcal{P}_{10,2013-2015,user}$ & $<$ 10m, same user & $<$ 10m & Jan 2013 - Dec 2015  & 118400\\
		$\mathcal{P}_{10,2013-2015,tl}$ & $<$ 10m & $<$ 10m, timelapse & Jan 2013 - Dec 2015  & \_\\
		$\mathcal{P}_{30,2013-2015,user}$ & $<$ 30m, same user within 2 hours & $<$ 30m & Jan 2013 - Dec 2015  & 198400\\
		$\mathcal{P}_{Middlebury}$ & Middlebury pairs & random & N/A & 32000 \\
		$\mathcal{P}_{1,2000,01\_2014}$ & $<$ 1m & $>$ 2000m & Jan 2014  & 32000\\
		$\mathcal{P}_{1,2000,02\_2014}$ & $<$ 1m & $>$ 2000m & Feb 2014  & 32000\\
		$\mathcal{P}_{1,2000,03\_2014}$ & $<$ 1m & $>$ 2000m & Mar 2014 & 32000\\
		$\mathcal{P}_{1,2000,04\_2014}$ & $<$ 1m & $>$ 2000m & Apr 2014  & 32000\\
		$\mathcal{P}_{1,2000,05\_62014}$ & $<$ 1m & $>$ 2000m & May 2014  & 32000\\
		$\mathcal{P}_{1,2000,06\_2014}$ & $<$ 1m & $>$ 2000m & Jun 2014  & 32000\\
		$\mathcal{P}_{1,2000,07\_2014}$ & $<$ 1m & $>$ 2000m & Jul 2014  & 32000\\
		$\mathcal{P}_{1,2000,08\_2014}$ & $<$ 1m & $>$ 2000m & Aug 2014  & 32000\\
		$\mathcal{P}_{1,2000,09\_2014}$ & $<$ 1m & $>$ 2000m & Sep 2014  & 32000\\
		$\mathcal{P}_{1,2000,10\_2014}$ & $<$ 1m & $>$ 2000m & Oct 2014  & 32000\\
		$\mathcal{P}_{1,2000,11\_2014}$ & $<$ 1m & $>$ 2000m & Nov 2014  & 32000\\
		$\mathcal{P}_{1,2000,12\_2014}$ & $<$ 1m & $>$ 2000m & Dec 2014  & 32000\\
		$\mathcal{P}_{1,2000,01\_2015}$ & $<$ 1m & $>$ 2000m & Jan 2015  & 32000\\
		$\mathcal{P}_{1,2000,02\_2015}$ & $<$ 1m & $>$ 2000m & Feb 2015  & 32000\\
		$\mathcal{P}_{1,2000,03\_2015}$ & $<$ 1m & $>$ 2000m & Mar 2015  & 32000\\
		$\mathcal{P}_{1,2000,04\_2015}$ & $<$ 1m & $>$ 2000m & Apr 2015  & 32000\\
		$\mathcal{P}_{1,2000,05\_2015}$ & $<$ 1m & $>$ 2000m & May 2015  & 32000\\
		$\mathcal{P}_{1,2000,06\_2015}$ & $<$ 1m & $>$ 2000m & Jun 2015  & 32000\\
		$\mathcal{P}_{1,2000,07\_2015}$ & $<$ 1m & $>$ 2000m & Jul 2015  & 32000\\
		$\mathcal{P}_{1,2000,08\_2015}$ & $<$ 1m & $>$ 2000m & Aug 2015  & 32000\\
		$\mathcal{P}_{1,2000,09\_2015}$ & $<$ 1m & $>$ 2000m & Sep 2015  & 32000\\
		$\mathcal{P}_{1,2000,10\_2015}$ & $<$ 1m & $>$ 2000m & Oct 2015  & 32000\\
		$\mathcal{P}_{1,2000,11\_2015}$ & $<$ 1m & $>$ 2000m & Nov 2015  & 32000\\
		$\mathcal{P}_{1,2000,12\_2015}$ & $<$ 1m & $>$ 2000m & Dec 2015  & 32000\\
		\bottomrule
	\end{tabular}
	\caption{Validation losses}
	\label{table:val_loss}
\end{table}

We initially expected $T_c_^1$ to outperform $T_c^2$, because in the case that the invariant portion of the output vector is highly similar for a pair, that indicates a feature detectable by an ImageNet trained model is highly prominent and this additional information should be highly useful. Essentially, only using a variant detecting output would limit us to predicting a correct ranking if $p$ and $p^+$ share more similar stylistic characteristics than $p$ and $p^-$. It's very likely that none of $p$, $p^+$, and $p^-$ have a particularly salient style, in which case the Euclidean distances of the variant portion of our embeddings will be on average equidistant from $p$ for both $p^+$ and $p^-$. In this case, the addition of the variant portion should allow us to make additional correct rankings since we expect geographically close images to have similar structures and textures, such as showing the sides of glass office buildings as contrasted with grass and trees in a park. When segmenting pairs by user, we would expect the same since photos drawn from the same user a likely to be of the same attraction. However, we actually find that the loss space for $T_c^1$ is difficult to optimize over. Across the board, $T_c^1$ trained models do not reach the same validation loss as do $T_c^2$ models, as shown in Table \ref{table:val_loss}. 

We conjecture that this is because we were unable to train our models on enough data or for enough epochs. In theory, the addition of features should never decrease a model's effectiveness, since any weighting of the smaller set of features will be a subset of the possible weightings of the larger set, with optimization solutions being equivalent when the extra features are given weights of 0. Reaching this solution, however, might take a prohibitively long time. We also note that in this formulation. We note also that there is potential for large errors in the magnitude of our weight updates to the variant model. Should the invariant portions of our output vector be dramatically different, this will cause the overall Euclidean distance of our prediction to be large as well. But since we are only updating the variant model, we will change its weights in an unwarranted manner. 

Our full model, as described in  includes a blending network $B$, which consists of one or two fully connected layers and takes as input the concatenated output vector from the variant and invariant model. We do a rough grid search for reasonable widths for layers $L_1$ and $L_2$, eventually deciding that TODO: insert parameters, with a dropout layer of TODO dropout, in between represented a reasonable architecture.

Our final model thus concatenates an output vector for variant features with a vector for invariant features and weights their indices according to the weights learned in the training of $B$. 



The ensemble test accuracies are reported in

\section{Investigation of Data Fuzziness}



\section{Image Invariants}





























