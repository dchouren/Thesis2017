\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Preface}{1}{chapter*.2}}
\@writefile{toc}{\contentsline {chapter}{Nested Statistical Models}{2}{chapter*.3}}
\citation{deepdriving}
\citation{deepdriving}
\citation{deepdriving}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{3}{section.1.1}}
\newlabel{intro}{{1.1}{3}{Introduction}{section.1.1}{}}
\citation{deepdriving}
\citation{deepdriving}
\citation{aic_bic1}
\citation{aic_bic2}
\citation{liew}
\citation{lenet}
\citation{mars}
\citation{fastmars}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Related Works}{4}{subsection.1.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Paper Organization}{4}{subsection.1.1.2}}
\citation{lenet}
\citation{nolearn}
\citation{lasagne}
\citation{theano}
\citation{statsmodels}
\citation{scikit-learn}
\citation{py-earth}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Data Wrangling and Tools}{5}{section.1.2}}
\newlabel{sec:data_tools}{{1.2}{5}{Data Wrangling and Tools}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Data Pipeline}{5}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Feature Extraction}{5}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Training and Testing}{5}{subsection.1.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Single History Inclusion}{6}{section.1.3}}
\newlabel{sec:single_history}{{1.3}{6}{Single History Inclusion}{section.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Lag Order Selection}{6}{section.1.4}}
\newlabel{sec:lag_order}{{1.4}{6}{Lag Order Selection}{section.1.4}{}}
\citation{mars}
\citation{fastmars}
\citation{earth}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Nonlinear Models}{8}{section.1.5}}
\newlabel{sec:nonlinear}{{1.5}{8}{Nonlinear Models}{section.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Basis Expansion}{8}{subsection.1.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Multivariate Adaptive Regressive Splines}{8}{subsection.1.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Results}{9}{section.1.6}}
\newlabel{sec:results}{{1.6}{9}{Results}{section.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}History Inclusion}{9}{subsection.1.6.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces p-value summary for Model 1 $\bm  {\beta }$ coefficients.}}{9}{table.1.1}}
\newlabel{table:p1}{{1.1}{9}{p-value summary for Model 1 $\bm {\beta }$ coefficients}{table.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces p-value summary for Model 2 $\bm  {\beta _t}$ coefficients.}}{10}{table.1.2}}
\newlabel{table:p2}{{1.2}{10}{p-value summary for Model 2 $\bm {\beta _t}$ coefficients}{table.1.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces p-value summary for Model 2 $\bm  {\beta _{t-1}}$ coefficients.}}{10}{table.1.3}}
\newlabel{table:p3}{{1.3}{10}{p-value summary for Model 2 $\bm {\beta _{t-1}}$ coefficients}{table.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The distributions of the 500 $t-1$ step p-values in our Model 2 structure. For each of the 14 models, each corresponding to a response variable, these histograms show that nearly half of the p-values fall in the 0.05 bucket. This indicates that past history, in the form of the previous time step's convolute image features, is statistically significant for prediction within the model.}}{11}{figure.1.1}}
\newlabel{fig:p_dist}{{1.1}{11}{The distributions of the 500 $t-1$ step p-values in our Model 2 structure. For each of the 14 models, each corresponding to a response variable, these histograms show that nearly half of the p-values fall in the 0.05 bucket. This indicates that past history, in the form of the previous time step's convolute image features, is statistically significant for prediction within the model}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Lag Order Selection}{12}{subsection.1.6.2}}
\newlabel{sec:lag-order-results}{{1.6.2}{12}{Lag Order Selection}{subsection.1.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Stacked AIC and BIC for each additional time step inclusion.}}{12}{figure.1.2}}
\newlabel{fig:stacked_ic}{{1.2}{12}{Stacked AIC and BIC for each additional time step inclusion}{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Aggregated RMSE for each additional time step inclusion.}}{12}{figure.1.3}}
\newlabel{fig:rmse}{{1.3}{12}{Aggregated RMSE for each additional time step inclusion}{figure.1.3}{}}
\citation{aic_bic1}
\citation{aic_bic2}
\citation{liew}
\citation{earth}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.3}Nonlinear Models}{13}{subsection.1.6.3}}
\@writefile{toc}{\contentsline {subsubsection}{Basis Expansion}{13}{section*.4}}
\@writefile{toc}{\contentsline {subsubsection}{MARS}{13}{section*.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Many different values of $\alpha $ are used for fitting the basis-expanded model during SGD, but none of the resulting models achieve an RMSE lower than the highest-performing linear models. Moreover, different iterations of a model trained using a given $\alpha $ (for example, $\alpha = 160$) give largely variable RMSE scores.}}{14}{figure.1.4}}
\newlabel{fig:basis_results}{{1.4}{14}{Many different values of $\alpha $ are used for fitting the basis-expanded model during SGD, but none of the resulting models achieve an RMSE lower than the highest-performing linear models. Moreover, different iterations of a model trained using a given $\alpha $ (for example, $\alpha = 160$) give largely variable RMSE scores}{figure.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces RMSE and training time of MARS models for various values of ``fast K''. Even the lowest values of ``fast K'' result in RMSE lower than the linear and basis-expanded models, while increasing ``fast K'' brings an even more drastic decrease (albeit at the expense of longer training times).}}{14}{figure.1.5}}
\newlabel{fig:mars_rmse}{{1.5}{14}{RMSE and training time of MARS models for various values of ``fast K''. Even the lowest values of ``fast K'' result in RMSE lower than the linear and basis-expanded models, while increasing ``fast K'' brings an even more drastic decrease (albeit at the expense of longer training times)}{figure.1.5}{}}
\citation{deepdriving}
\citation{deepdriving}
\citation{deepdriving}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Discussion}{15}{section.1.7}}
\newlabel{sec:discussion}{{1.7}{15}{Discussion}{section.1.7}{}}
\@writefile{toc}{\contentsline {chapter}{Nonparametric Long Short-Term Memory Neural Networks}{16}{chapter*.6}}
\citation{deepdriving}
\citation{lenet}
\citation{lstm_hochreiter}
\citation{hochreiter_analysis}
\citation{lstm_hochreiter}
\citation{lrcn2014}
\citation{chenyi_phd}
\citation{deepdriving}
\citation{chenyi_phd}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{17}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}TORCS, Direct Perception, and Recurrency}{17}{subsection.2.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}LSTM in Caffe}{17}{subsection.2.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Chen's TORCS 2-Layer Skipped LSTM Structure. Images are fed into convolutional layers, followed by a fully connected activation. These, in turn, feed into the second of two LSTM modules (LSTM 2), as does the first of the LSTM modules (LSTM 1) and the LSTM 2 of the previous time step. LSTM 1 also receives input from the LSTM 1 and the affordance indicator output at the previous time step. LSTM 2 is then followed by another fully connected layer, which produces the final output response, the 13 affordance indicators. Note that in Part 1, a 14th response variable pointing to TORCS video clip endings was used, but it is omitted in the output of this model.}}{18}{figure.2.6}}
\newlabel{fig:chenyi_lstm}{{2.6}{18}{Chen's TORCS 2-Layer Skipped LSTM Structure. Images are fed into convolutional layers, followed by a fully connected activation. These, in turn, feed into the second of two LSTM modules (LSTM 2), as does the first of the LSTM modules (LSTM 1) and the LSTM 2 of the previous time step. LSTM 1 also receives input from the LSTM 1 and the affordance indicator output at the previous time step. LSTM 2 is then followed by another fully connected layer, which produces the final output response, the 13 affordance indicators. Note that in Part 1, a 14th response variable pointing to TORCS video clip endings was used, but it is omitted in the output of this model}{figure.2.6}{}}
\citation{chenyi_phd}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Methods}{19}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Nonparametric ReLU}{19}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Parametric Initialization}{19}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Results}{20}{section.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Validation loss of the parametric and sigmoid nonparametric training.}}{20}{figure.2.7}}
\newlabel{fig:sigmoid}{{2.7}{20}{Validation loss of the parametric and sigmoid nonparametric training}{figure.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Validation loss of the parametric and Fourier nonparametric training.}}{20}{figure.2.8}}
\newlabel{fig:sincos}{{2.8}{20}{Validation loss of the parametric and Fourier nonparametric training}{figure.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Validation loss of the parametric initialization, and both sigmoid and Fourier nonparametric training. The majority of the loss decrease is obtained during the parametric initialization, with the nonparametric networks' loss remaining around the same values as those of the parametric initialization. Neither the sigmoid nor the Fourier-based models show stronger performance than the other, both achieving validation loss convergence at a similar level.}}{21}{figure.2.9}}
\newlabel{fig:np_result}{{2.9}{21}{Validation loss of the parametric initialization, and both sigmoid and Fourier nonparametric training. The majority of the loss decrease is obtained during the parametric initialization, with the nonparametric networks' loss remaining around the same values as those of the parametric initialization. Neither the sigmoid nor the Fourier-based models show stronger performance than the other, both achieving validation loss convergence at a similar level}{figure.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Discussion}{22}{section.2.4}}
\@writefile{toc}{\contentsline {chapter}{deepbayes: An R Package for Deep Learning and Bayesian Optimization}{23}{chapter*.7}}
\citation{h2o}
\citation{bayesopt_paper}
\citation{rcpp}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{24}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Bayesian Optimization}{24}{subsection.3.1.1}}
\newlabel{sec:bo}{{3.1.1}{24}{Bayesian Optimization}{subsection.3.1.1}{}}
\citation{jia2014caffe}
\citation{bayesopt}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Technical Choices}{25}{subsection.3.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\lstinline {Rcpp}}{25}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{\lstinline {Caffe}}{25}{section*.9}}
\@writefile{toc}{\contentsline {subsubsection}{BayesOpt}{25}{section*.10}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Package Usage}{25}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Data}{26}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Creating Network and Solver Protocol Buffers}{26}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Optimizing}{26}{subsection.3.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Other Examples}{26}{subsection.3.2.4}}
\bibstyle{ims}
\bibdata{RefDatBas}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Discussion and Future Work}{27}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}External vs. Internal Libraries}{27}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Abstraction of Protocol Buffer Creation}{27}{subsection.3.3.2}}
\newlabel{sec:db_abstr}{{3.3.2}{27}{Abstraction of Protocol Buffer Creation}{subsection.3.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Bayesian Optimization Parameters}{27}{subsection.3.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Conclusion}{27}{section.3.4}}
\bibcite{h2o}{{1}{2015}{{Aiello et~al.}}{{Aiello, Kraljevic and Maj}}}
\bibcite{theano}{{2}{2010}{{Bergstra et~al.}}{{Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley and Bengio}}}
\bibcite{aic_bic1}{{3}{2004}{{Burnham and Anderson}}{{}}}
\bibcite{chenyi_phd}{{4}{2016}{{Chen}}{{}}}
\bibcite{deepdriving}{{5}{2015}{{Chen et~al.}}{{Chen, Seff, Kornhauser and Xiao.}}}
\bibcite{lasagne}{{6}{2012}{{Dieleman et~al.}}{{Dieleman, Raffel, Olson, Schlüter and Sønderby}}}
\bibcite{lrcn2014}{{7}{2015}{{Donahue et~al.}}{{Donahue, Hendricks, Guadarrama, Rohrbach, Venugopalan, Saenko and Darrell}}}
\bibcite{rcpp}{{8}{2011}{{Eddelbuettel and Fran\c {c}ois}}{{}}}
\bibcite{mars}{{9}{1991}{{Friedman}}{{}}}
\bibcite{fastmars}{{10}{1993}{{Friedman}}{{}}}
\bibcite{hochreiter_analysis}{{11}{1991}{{Hochreiter}}{{}}}
\bibcite{lstm_hochreiter}{{12}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{jia2014caffe}{{13}{2014}{{Jia et~al.}}{{Jia, Shelhamer, Donahue, Karayev, Long, Girshick, Guadarrama and Darrell}}}
\bibcite{aic_bic2}{{14}{2004}{{Kuha}}{{}}}
\bibcite{lenet}{{15}{1998}{{LeCun et~al.}}{{LeCun, Bottou, Bengio and Haffner}}}
\bibcite{liew}{{16}{2004}{{Liew}}{{}}}
\bibcite{bayesopt}{{17}{2015}{{Martinez-Cantin}}{{}}}
\bibcite{earth}{{18}{2016}{{Milborrow}}{{}}}
\bibcite{nolearn}{{19}{2012}{{Nouri}}{{}}}
\bibcite{scikit-learn}{{20}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot and Duchesnay}}}
\bibcite{statsmodels}{{21}{2009}{{Perktold et~al.}}{{Perktold, Seabold and Taylor}}}
\bibcite{py-earth}{{22}{2013}{{Rudy}}{{}}}
\bibcite{bayesopt_paper}{{23}{2015}{{Shahriari et~al.}}{{Shahriari, Swersky, Wang, Adams and de~Freitas}}}
