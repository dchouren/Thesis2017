
\section{A History of Image Similarity}

Before we discuss the history of image similarity and image understanding research, we return briefly to the semantics of what makes two images similar. Humans view images as a collection of high level features. An image of a bound thesis might be similar to an image of a cyclist winning the Tour de France. It might also be considered similar to images of Prometheus chained to the side of a cliff. In order for a model to learn these high level features, however, it must first be able to learn the low level features that describe the structures of an image and not necessarily what am image is about or even what is in an image. The jump from recognizing low level features like colors and edges to recognizing concepts like suffering at 5:21am, has been dubbed "the semantic gap."

In 1999, John Eakins summarized the hierarchy of detectable features in the field of content-based image retrieval, whose goals are similar to those of image similarity.\cite{eakins1999university} Level one content-based retrieval system supports the retrieval of images based on features such as color, texture, shape, and spatial locations of objects. Level two allows for queries of complex aggregations of features that are typically nameable by humans, such as "find images that contain dogs." Level three requires an image retrieval system capable of inferring a semantic quality about a scene, such as, "Is this restaurant romantic?"

The semantic gap has still not been bridged. Work in the earliest days of machine vision was focused on extracting level one features and attempted to aggregate these into level two descriptors. Today, a deep learning revolution has brought the field to the cusp of level three systems, but as system have become more sophisticated, the difficulty of achieving incremental gains has also increased. We will briefly discuss low-level descriptors and some classical techniques used to extract them before formalizing some of the deep learning research that serves as the direct base for our work.

\subsection{Low-Level Descriptors and Classical Techniques}
We present a general set of low-level feature descriptors in Table \ref{table:descriptors}. This overview is far from comprehensive but gives a general idea of the classical features used in image similarity and other image understanding research.
\begin{table}[h]
	\centering
	%     \begin{adjustbox}{width=\textwidth,center}
	% \begin{adjustbox}{center}
	\begin{tabular}{c p{3cm} >{\centering\arraybackslash}m{3in} }
		\toprule
		\multirow{5}{*}{Color \cite{ohm2001mpeg}} & \multicolumn{1}{l}{Dominant Color} & \multicolumn{1}{l}{Statistical properties of dominant colors} \\\cline{2-3}
		& \multicolumn{1}{l}{Scalable Color} & \multicolumn{1}{l}{Color histogram in HSV space with fixed quantization} \\\cline{2-3}
		& \multicolumn{1}{l}{Group of Frames} & \multicolumn{1}{l}{Extension of scalable color to groups of pictures} \\\cline{2-3}
		& \multicolumn{1}{l}{Color Structure} & \multicolumn{1}{l}{Localized color distribution in Hue-Min-Max-Difference space} \\\cline{2-3}
		& \multicolumn{1}{l}{Color Layout} & \multicolumn{1}{l}{Gridded layout of dominant colors} \\\hline
		\multirow{3}{*}{Texture} & \multicolumn{1}{l}{Homogenous Texture} & \multicolumn{1}{l}{Image Fourier transform statistics \cite{ro2001mpeg}} \\\cline{2-3}
		& \multicolumn{1}{l}{Texture Browsing} & \multicolumn{1}{l}{Directionality, regularity, coarseness of textures \cite{manjunath2001color}} \\\cline{2-3}
		& \multicolumn{1}{l}{Edge Histogram} & \multicolumn{1}{l}{Frequency and directionality of brightness changes \cite{won2002efficient}}  \\\hline
		\multirow{2}{*}{Shape} & \multicolumn{1}{l}{Region-based Shape} & \multicolumn{1}{l}{Pixel distribution within a 2D object region \cite{bober2001mpeg}} \\\cline{2-3}
		& \multicolumn{1}{l}{Contour-based Shape} & \multicolumn{1}{l}{Distribution of pixels within object contour \cite{bober2001mpeg}} \\\hline
		\multirow{2}{*}{Location} & \multicolumn{1}{l}{Region Locator} & \multicolumn{1}{l}{Element location in image} \\\cline{2-3}
		& \multicolumn{1}{l}{Spatio Temporal Locator} & \multicolumn{1}{l}{Element location in temporal domain} \\  
		\bottomrule
	\end{tabular}
	%     \end{adjustbox}
	%     \vspace{ - 05 mm}
	\caption{Some general image descriptors}
	\label{table:descriptors}
\end{table}

Color-based features are perhaps the easiest to define. Features are represented in various color spaces that are thought to be close to human perception. These include RBG, LAB, LUV, HSV, YCrCb, and HMMD spaces. For instance, the scalable color descriptor, which uses a Haar transform to allow it to scalably represent features, is computed in Hue-Saturation-Value (HSV) space and the color structure descriptor, which uses a small structuring window to localize color distributions, is computed in Hue-Min-Max-Difference (HMMD) space. Within these spaces, many other statistics such as color-covariance matrices and color histograms can be computed. 

Texture features are often obtained through Gabor filters, wavelets, and local statistical methods. Gabor filters in particular prove useful for edge detection and texture segmentation.\cite{weldon1996efficient}\cite{mehrotra1992gabor} Statistical methods like Gaussian random Markov fields are used to measure local pixel interdependencies as a way of identifying textures.\cite{porter1997robust} The Tamura features detect coarseness, contrast, directionality, linelikeness, regularity, and roughness by computing statistics like frequency distributions and moments of pixel gray values.\cite{tamura1978textural}

Statistical approaches are also used for shape-based features. For example, Fourier descriptors determine Fourier transformed boundaries and Delaunay triangulation is used for discovering image segments.

The descriptors for low-level features share a manual definition. Using a variety of these techniques allows for the creation of a bag-of-visual-words (BOV) representing different characteristics about color, shape, texture, image segments, and feature locations, and from there, an image can be described on a global level using more complex and often spatially hierarchical algorithms, which take into consideration patch-level statistics about features, spatial relations between local features, and other aggregated computations. For instance, Bouchard and Triggs note that many objects have rigid structures on small local levels. Local rigidity can be detected using combinations of edge and texture finding algorithms, but in order to capture the variance of the shape of these objects on a larger scale, they used an expectation maximization algorithm to recursively assign extracted local features to a hierarchy of parent classes.\cite{bouchard2005hierarchical} In a similar vein, Grauman and Darrell extract local features and use spatial neighborhood constraints to compare the overall distribution of locally detected features between images.\cite{grauman2005efficient} Other hierarchical feature representations, such as spatial pyramids, which transform images into locally oriented segmentations proved effective in a variety of image tasks as well\cite{yang2009linear}\cite{girshick2014rich}\cite{lazebnik2006beyond}. Another method of moving from local descriptors to more complex global descriptors that does not rely on hierarchically aggregating or comparing BOVs, is the fisher vector. Rather than quantize local visual words, the Fisher vector assume features are generated from a Gaussian mixture model and describes local image areas by their deviation from an expected feature distribution.\cite{sanchez2013image}

Some of the most notable feature extractors proved useful because of their invariance under image transformations. Histograms of oriented gradients (HOGs), which compute local intensity gradients and edge directions across a tiled image, proved adept at detecting humans in photos.\cite{dalal2005histograms} HOGs operate on local tiles, so they are invariant to transformations of the global image except for relative orientations of the object to be detected, and this proved useful for humans, which are usually upright. Similarly scale-invariant feature transforms, which detects scale, rotation, and illumination invariant local features, has proven especially useful in object recognition, 3D modeling, and robotic navigation.\cite{lowe1999object} Further work was done to extend SIFT to include other invariances, such as color (CSIFT) and geometric scale (GSIFT).\cite{abdel2006csift}\cite{lodha2005gsift}

Overall, between work done in aggregating local descriptors into global features and in detecting invariant features, by 2012, level 2 image understanding had been achieved with reasonable success. The ImageNet classification challenge asks participants to classify images into one of 1000 possible classes, and by 2012, a weighted prediction between SIFT, CSIFT, GSIFT, and Fisher vectors was able to do so with a top-5 error of just 26.2\%, where top-5 error is defined as the percentage of samples for which none of a model's top five guesses match the true label.\cite{russakovsky2015imagenet}

\subsection{The Deep Learning Revolution}
The fields of image similarity and image understanding were completely reset in 2012 by the submission of Krizhevsky et al.'s deep neural network to the 2012 ILSVRC ImageNet classification competition. Krizhevsky et al.'s model scored a top-5 error rate of 15.3\%, which was almost 70\% better than the weighted prediction by SIFT, CSIFT, GSIFT, and Fisher vectors.\cite{krizhevsky2012imagenet}.

Krizhevsky et al.'s performance demonstrated that convolutional neural networks, which had been applied by LeCunn et al. in 1989 for handwriting recogniztion, and which had been theorized as capable of learning low, medium, and high level features by using nonlinear transformations, were feasible for use in general image understanding problems.\cite{lecun1989backpropagation}


\subsection{Deep Convolutional Neural Networks}
Convolutional neural networks have become the de-facto standard in image tasks, as stacked convolutional layers are well suited for learning image descriptors.\cite{karpathy2014large}\cite{krizhevsky2012imagenet}\cite{szegedy2015going} Besides convolutional layers, CNNs typically consist of pooling layers, activation layers, fully connected layers, and a loss layer. 

\subsubsection{Convolutional Layers}
A convolutional layer consists of a set of kernels $K$, each of which typically has width and height dimensions smaller than the dimensions of an inputted image, but with a depth matching the depth of the input. During the forward pass of network training, each filter is convolved with a sliding patch across the input's width and height, producing a feature map associated with that kernel. Each feature map codes the activation of the kernel along with the spatial location of that activation. Because the dimensions of $K$ are smaller than the input, convolutional layers only have local connectivity.

\subsubsection{Pooling Layers}
Pooling layers, usually in the form of max-pooling, down sample the feature maps produced by the convolutional layers. Max pooling will output the maximum value in each part of a segmentation of a feature map. Pooling layers drastically reduce the computation required to train a network. 

\subsubsection{Activation Layers}
Activation layers are used to control which nodes in a layer send output to the next layer. The standard activation currently used is the rectified linear activation unit (ReLU), which takes the form
\begin{equation}
f(x) = \begin{cases}
x & x > 0 \\
0 & x\leq 0 \\
\end{cases}
\end{equation}
Various forms of ReLU have been proposed, notably the parametric ReLU (pReLU), which adds a learnable parameter, $\alpha$, to control a slope for the negative activation domain and which was used by He et al. to achieve better than human performance for ImageNet classification.\cite{he2016deep}
\begin{equation}
f(x) = \begin{cases}
x & x > 0 \\
\alpha x & x \leq 0 \\
\end{cases}
\end{equation}

Since convolutional layers only produce feature maps on local scales, fully connected layers at the end of a CNN allow for high level features to be learned. These layers have full connections to all activated neurons from previous layers, which allow for global mixing of activated feature maps. To complete our discussion of the CNN, the loss layer specifies how a network should penalize incorrect predictions. Typically sigmoid cross-entropy loss is used. Regularization is also used, typically in the form of L1 or L2 weight decay. Dropout layers, which deactivate a random subset of a layer's neurons in each iteration of training, have also proved highly effective for neural network regularization.\cite{srivastava2014dropout}

The term deep CNN refers to a CNN that has many layers. This definition is highly variable: the popular VGG16 model has 16 layers\cite{simonyan2014very} and popular versions of ResNet contain 34, 50, 101, and 152 layers\cite{he2016deep}.

We can more precisely define a CNN as a function $f$ that takes parameters $\theta$ and an input image $I$ to produce an image embedding $x$ and a loss $L$.

\subsection{Deep Networks Today}
Recent research has shown that increasing the depth of deep neural networks increases their performance.\cite{szegedy2015going} Much work on different activation functions has allowed CNNs to become sparser and thus take up less memory despite increasing depth.\cite{simonyan2014very}\cite{szegedy2015going} Some research has focused on allowing models to regulate their own depth as well.\cite{he2016deep} In combination, these advancements have allowed networks that are hundreds of layers deep to become standard, and these extremely deep models have continued to excel.  In recent years, some deep learning models have even achieved classification accuracies surpassing even human performance. In 2015, He et al. achieved a 4.94\% top-5 test error on the ImageNet 2012 dataset, surpassing the human error performance of 5.1\%\cite{he2016deep}. 

The features learned by deep learning models have proAthiwaratkun and Kang show that just the image representation extracted by deep CNNs can be combined with simpler classifiers such as SVMs and random forests to achieve high accuracies for clustering tasks.\cite{athiwaratkun2015feature}

Importantly, in a departure from classical image processing techniques, deep learning methods do not require the manual crafting of features based on domain-level knowledge. Instead, deep learning models learn to abstract patterns from data automatically. It has been shown through visualizations of the weights learned by deep networks that these network learn many of the same low-level features as do classical techniques. Typically the first few layers of a network will learn various edge detection filters. Many of these filters will be replicated throughout the network in slightly different fashions, which show that networks learn invariance for basic descriptors. And similarly to how much research in the 2000s focused on aggregating counts of local feature descriptors into larger parent patches of the image, many deep networks include pooling and fully connected layers, which effectively spread knowledge about locally detected features to neighboring patches of the network. 


\subsection{Image Invariants}
Ever since Krizhevsky et al. popularized the use of deep CNNs in 2012, researchers have focused on ways of incorporating various image invariants in their deep models. Invariance is learned when many examples of the same class are fed to a model, forcing the model to generalize its learned representation of that class to ignore minor differences in the image. In practice, this is often achieved through data augmentation, through flipping, rotating, shearing, zooming, and changing color intensities in images. But in many cases, datasets either already contain enough images that capture certain invariants, or datasets are curated with the goal of including certain invariances. For example, Hadsell et al. learn an invariant mapping for dimensionality reduction by taking images of a plane from a full span of 360$\deg$ angles.\cite{hadsell2006dimensionality} Regardless of how invariance finds its way into the dataset, it has become standard for many tasks to verify that invariance is being learned. In demonstrating that an unsupervised model could detect the presence of high level features such as cat faces, Le et al. also use an out-of-plane rotation dataset to confirm that their model learned rotational invariance, which was particularly important since cat faces are often turned to the side.\cite{le2013building}

It is said that CNNs have translational invariance baked into their architecture. As CNNs apply kernels independently of location across input images, is it impossible for the CNNs not to learn translational invariance of kernel activations. Lenc and Vedaldi's study on invariants in deep networks concluded that almost all classification datasets contain enough examples with naturally occurring horizontal flips and rescaling, that these invariants are usually learned as well.\cite{vedaldi2010vlfeat} Rotational and vertical flip invariance is less commonly seen.

While the emphasis on learning invariants is not new or unique to deep learning, there seems to be an intense focus on invariants perhaps because learning these basic invariants does not require any special architecture or training regime.

\section{Deep Learning for Image Similarity}
\subsection{Siamese Neural Networks}

As noted by Wang et al\cite{wang2014learning}, the network structures that are effective at classifying images into object classes are not necessarily well-designed for detecting image similarity, especially when image similar is defined not just as whether two objects are in the same class, but when our desired similar metric must be fine enough to rank similarities within classes. The Siamese architecture, first proposed by Bromley et al. in 1994\cite{bromley1993signature} for the purposes of verifying signatures, is well suited for this task. Siamese networks have since been used in a variety of similarity tasks, such as ground-to-aerial geolocalization by Lin\cite{lin2015learning}, matching visual similarity for product design by Bell and Bala\cite{bell2015learning}, comparing image patches\cite{zagoruyko2015learning}, and one-shot image classification by Koch\cite{koch2015siamese}.

A Siamese net is a formulation of at least two copies of a CNN that share parameters and hyperparameters as well as a loss layer and thus weight updates. If we represent a Siamese net as $f$, then $f$ takes network weights $\theta$, two images \textbf{$I_1, I_2$} as well as an indicator variable $p$ to indicate if these images form a positive (similar) or negative (dissimilar) pair, produces two embeddings $x_1, x_2$ and one loss $L$. We wish to find a locally optimal $\theta$ such that for a triplet of embeddings $x_1, x_2, x_3$ produced by $f$, if $|x_1 - x_2|_2^2 < |x_1 - x_3|_2^2$, then we expect \textbf{$I_1$} and \textbf{$I_2$} to be much more semantically similar than \textbf{$I_1$} and \textbf{$I_3$}.

As discussed by Bell and Bala\cite{bell2015learning}, Siamese networks can be tweaked in various ways to produce both an embedding and a classification for each pair of images. The output from the CNN layer can regularized either for the embedding predictions or the class predictions or both. If there are multiple outputs from the CNN layer, multiple loss layers can be used as well.

For the purposes of learning image similarity, only a simple formulation is required. The embeddings produced by each half of the Siamese network must be compared in some manner, and a standard Euclidean distance is usually used. Formally, we define the similarity of two images, $S(A,B)$, as the Euclidean distance of their feature embedded vectors, $f(A)$ and $f(B)$:
\begin{equation}
S(A,B) = ||f(A) - f(B)||_2^2
\end{equation}
A cosine similarity $\dfrac{f(A) \cdot f(B)}{||f(A)||||f(B)||}$ can also be used but this is less common. We note that though in the simple Siamese formula discussed here, $f(\cdot)$ is taken as the weight representation of the final convolutional block of a single CNN, we can construct a Siamese net that runs each image through mutiple CNNs or other feature extraction modules. In this case, $f(\cdot)$ would refer to the aggregate feature embedding found by all the modules. Embeddings might be aggregated via concatenation or another more complicated technique.

We base our network design, which we discuss in \ref{chapter:network}, off of a fairly complex Siamese network with two different feature extracting modules and a blending model for their aggregation.


\section{Weakly Supervised Learning}
The problems with relying on supervised learning are many. It is difficult and time consuming to create large datasets, and even when these are created, such as for ImageNet, they are often specific to a certain task and do not necessarily extend to novel concepts. As Russakowsky et al. discussed\cite{russakovsky2013detecting}, models pretrained on general datsets like ImageNet were only good at dividing images into basic classes. Russakowsky et al. also demonstrated that these models were better tuned for classification of natural classes such as animals than they were for man-made objects, suggesting that even an expansive 12 million image-large dataset like ImageNet still had flaws for general classification training. In recent years, a proliferation of intra-class datasets, such as for hundreds of species of flowers or birds, have allowed deep learning techniques to tackle everything from differentiating species of flowers\cite{angelova2013image}, leaves\cite{rejeb2013vantage}, and birds\cite{berg2014birdsnap}, but the limitations of a class-based formulation of image similarity remain apparent. More and more specific class formulas will need to be created, and this is higly reliant on how explicit human annotations are. For example, labeling an image as a dog, which might be reasonable for an animal differentiation task, will create problems if this data is ever used for a more fine-grained image similarity task, such as one that requires differentiation of labradors from golden retrievers.

Much work on using weakly supervised learning has focused on using web images as a source of easily and quickly obtainable weakly labeled dataset.\cite{bergamo2010exploiting}\cite{fergus2010learning}\cite{li2010optimol}\cite{schroff2011harvesting}. While on the whole successful, researchers note that web created classes suffer from polysemy and noise problems. A search for penguin images will not necessarily return only penguins because of the way images and surrounding text are indexed, and a search for screens might return both door screens and computer screen. In the field of object segmentations, Rubinstein et al. achieved better than state of the art benchmark results training on a web gathered corpus, yet also noted difficulty with certain classes because of the low quality of some images.\cite{rubinstein2013unsupervised}

Krause et al. use fuzzy web-based data to remarkable effectiveness, successfully training classification for over 14,000 image classes, achieving close to human accuracy on the CUB Caltech bird species dataset, and beating state of the art results for CUB and the Stanford dog dataset\cite{krause2016unreasonable}. They quantify the fuzziness (incorrectly labeled images) in their images classes to be an average of 16\%, demonstrating well-trained CNNs are still able to overcome significant fuzziness. 

