
\section{A History of Image Similarity}

In 1999, John Eakins summarized the hierarchy of detectable features in the field of content-based image retrieval, whose goals are similar to those of image similarity.\cite{eakins1999university} A level one content-based retrieval system supports the retrieval of images based on features such as color, texture, shape, and spatial locations of objects. Level two allows for queries of complex aggregations of features that are typically nameable by humans, such as ``find images that contain dogs.'' Level three requires an image retrieval system capable of inferring a semantic quality about a scene, such as, ``Is this restaurant romantic?'' There are no hard boundaries between these levels, but building a system on par with human perception requires jumping from recognizing low level features like colors and edges to recognizing concepts associated with a particular visual stimulus. This jump has been dubbed ``the semantic gap.'' 

The semantic gap has still not been bridged. Work in the earliest days of machine vision was focused on extracting level one features and attempted to aggregate these into level two descriptors. Today, a deep learning revolution has brought the field to the cusp of level three systems, but as system have become more sophisticated, the difficulty of achieving incremental gains has also increased. We will briefly discuss low-level descriptors and some classical techniques used to extract them before formalizing some of the deep learning research that serves as the direct base for our work.

\subsection{Low-Level Descriptors and Classical Techniques}
Table \ref{table:descriptors} contains a general set of low-level feature descriptors. This overview is far from comprehensive but gives a general idea of the types of low-level features used in image similarity and other image understanding research. 
\begin{table}[h]
	\centering
	%     \begin{adjustbox}{width=\textwidth,center}
	% \begin{adjustbox}{center}
	\begin{tabular}{c p{3cm} >{\centering\arraybackslash}m{3in} }
		\toprule
		\multirow{5}{*}{Color \cite{ohm2001mpeg}} & \multicolumn{1}{l}{Dominant Color} & \multicolumn{1}{l}{Statistical properties of dominant colors} \\\cline{2-3}
		& \multicolumn{1}{l}{Scalable Color} & \multicolumn{1}{l}{Color histogram in HSV space with fixed quantization} \\\cline{2-3}
		& \multicolumn{1}{l}{Group of Frames} & \multicolumn{1}{l}{Extension of scalable color to groups of pictures} \\\cline{2-3}
		& \multicolumn{1}{l}{Color Structure} & \multicolumn{1}{l}{Localized color distribution in Hue-Min-Max-Difference space} \\\cline{2-3}
		& \multicolumn{1}{l}{Color Layout} & \multicolumn{1}{l}{Gridded layout of dominant colors} \\\hline
		\multirow{3}{*}{Texture} & \multicolumn{1}{l}{Homogenous Texture} & \multicolumn{1}{l}{Image Fourier transform statistics \cite{ro2001mpeg}} \\\cline{2-3}
		& \multicolumn{1}{l}{Texture Browsing} & \multicolumn{1}{l}{Directionality, regularity, coarseness of textures \cite{manjunath2001color}} \\\cline{2-3}
		& \multicolumn{1}{l}{Edge Histogram} & \multicolumn{1}{l}{Frequency and directionality of brightness changes \cite{won2002efficient}}  \\\hline
		\multirow{2}{*}{Shape} & \multicolumn{1}{l}{Region-based Shape} & \multicolumn{1}{l}{Pixel distribution within a 2D object region \cite{bober2001mpeg}} \\\cline{2-3}
		& \multicolumn{1}{l}{Contour-based Shape} & \multicolumn{1}{l}{Distribution of pixels within object contour \cite{bober2001mpeg}} \\\hline
		\multirow{2}{*}{Location} & \multicolumn{1}{l}{Region Locator} & \multicolumn{1}{l}{Element location in image} \\\cline{2-3}
		& \multicolumn{1}{l}{Spatio Temporal Locator} & \multicolumn{1}{l}{Element location in temporal domain} \\  
		\bottomrule
	\end{tabular}
	%     \end{adjustbox}
	%     \vspace{ - 05 mm}
	\caption{Some general image descriptors}
	\label{table:descriptors}
\end{table}

Color-based features are perhaps the easiest to define. Features are represented in various color spaces that are thought to be close to human perception. These include RGB, LAB, LUV, HSV, YCrCb, and HMMD spaces. For instance, the scalable color descriptor, which uses a Haar transform to allow it to scalably represent features, is computed in Hue-Saturation-Value (HSV) space and the color structure descriptor, which uses a small structuring window to localize color distributions, is computed in Hue-Min-Max-Difference (HMMD) space. Within these spaces, many other statistics such as color-covariance matrices and color histograms can be computed. 

Texture features are often obtained through Gabor filters, wavelets, and local statistical methods. Gabor filters in particular prove useful for edge detection and texture segmentation.\cite{weldon1996efficient}\cite{mehrotra1992gabor} Statistical methods like Gaussian random Markov fields are used to measure local pixel interdependencies as a way of identifying textures.\cite{porter1997robust} The Tamura features detect coarseness, contrast, directionality, linelikeness, regularity, and roughness by computing statistics like frequency distributions and moments of pixel gray values.\cite{tamura1978textural}

Statistical approaches are also used for shape-based features. Fourier descriptors determine Fourier transformed boundaries and Delaunay triangulation is used for discovering image segments.

These descriptors for low-level features all require a manual definition. Using a variety of these techniques allows for the creation of a bag-of-visual-words (BOV) representing different characteristics about color, shape, texture, image segments, and feature locations, and from there, an image can be described on a global level using more complex and often spatially hierarchical algorithms, which take into consideration patch-level statistics about features, spatial relations between local features, and other aggregated computations. For instance, Bouchard and Triggs note that many objects have rigid structures on small local levels. Local rigidity can easily be detected using these low-level descriptors, but in order to capture the variance of the shape of objects on a larger scale, Bouchard and Triggs use an expectation maximization algorithm to recursively assign extracted local features to a hierarchy of parent classes.\cite{bouchard2005hierarchical} In a similar vein, Grauman and Darrell extract local features and use spatial neighborhood constraints to compare the overall distribution of locally detected features between images.\cite{grauman2005efficient} Other hierarchical feature representations, such as spatial pyramids, which transform images into locally oriented segmentations proved effective in a variety of image tasks as well\cite{yang2009linear}\cite{girshick2014rich}\cite{lazebnik2006beyond}. Another method of moving from local descriptors to more complex global descriptors that does not rely on hierarchically aggregating or comparing BOVs, is the fisher vector. Rather than quantize local visual words, the Fisher vector assume features are generated from a Gaussian mixture model and describes local image areas by their deviation from an expected feature distribution.\cite{sanchez2013image}

As some techniques became increasingly adept and effective as building global descriptors from local low-level features, other feature extractors gained widespread use because they preserved feature representations even under image transformations. Histograms of oriented gradients (HOGs), which compute local intensity gradients and edge directions across a tiled image, proved adept at detecting humans in photos.\cite{dalal2005histograms} HOGs operate on local tiles, so they are invariant to transformations of the global image except for relative orientations of the object to be detected, and this proved useful for humans, which are usually upright. Similarly scale-invariant feature transforms, which detects scale, rotation, and illumination invariant local features, has proven especially useful in object recognition, 3D modeling, and robotic navigation.\cite{lowe1999object} Further work has been done to extend SIFT to include other invariances, such as color (CSIFT) and geometric scale (GSIFT).\cite{abdel2006csift}\cite{lodha2005gsift}

Overall, between work done in aggregating local descriptors into global features and in detecting invariant features, by 2012, level 2 image understanding had been achieved with reasonable success. The ImageNet classification challenge asks participants to classify images into one of 1000 possible classes, and by 2012, a weighted prediction between SIFT, CSIFT, GSIFT, and Fisher vectors was able to do so with a top-5 error of just 26.2\%, where top-5 error is defined as the percentage of samples for which none of a model's top five guesses match the true label.\cite{russakovsky2015imagenet}

\subsection{The Deep Learning Revolution}
The fields of image similarity and image understanding were completely reset in 2012 by the submission of Krizhevsky et al.'s deep convolutional neural network to that same 2012 ILSVRC ImageNet classification competition. Krizhevsky et al.'s model scored a top-5 error rate of 15.3\%, which was almost 70\% better than the second place weighted prediction by SIFT, CSIFT, GSIFT, and Fisher vectors.\cite{krizhevsky2012imagenet}.

Krizhevsky et al.'s performance demonstrated that convolutional neural networks, which had been applied by LeCunn et al. in 1989 for handwriting recogniztion, and which had been theorized as capable of learning low, medium, and high level features by using nonlinear transformations, were feasible for use in general image understanding problems.\cite{lecun1989backpropagation}


\subsection{Deep Convolutional Neural Networks}
Convolutional neural networks have become the de-facto standard in image tasks, as stacked convolutional layers are well suited for learning image descriptors.\cite{karpathy2014large}\cite{krizhevsky2012imagenet}\cite{szegedy2015going} Besides convolutional layers, CNNs typically consist of pooling layers, activation layers, fully connected layers, and a loss layer. 

\subsubsection{Convolutional Layers}
A convolutional layer consists of a set of kernels $K$, each of which typically has width and height dimensions smaller than the dimensions of an inputted image, but with a depth matching the depth of the input. During the forward pass of network training, each filter is convolved with a sliding patch across the input's width and height, producing a feature map associated with that kernel. Each feature map codes the activation of the kernel along with the spatial location of that activation. Because the dimensions of $K$ are smaller than the input, convolutional layers only have local connectivity. An example of a kernel is the edge detection kernel:
\[
\begin{bmatrix}
	-1 & -1 & -1\\
	-1 & 8 & -1\\
	-1 & -1 & -1
\end{bmatrix}
\]
If a $5\times 5$ image is fed to a network that applies this kernel, a 3$\times$ 3 feature map will be returned, assuming it is specified that the kernel can only be applied to to patches of the image where signals overlap completely. Many other useful kernels exist, and by applying hundreds of these across input images, convolutional layers can detect basic evidence of edges, textures, and other low-level features. A kernel operating in the red channel of an image, for instance, will detect certain low-level color descriptors. Deep convolutional networks typically have hundreds of filters.

\subsubsection{Pooling Layers}
Pooling layers, usually in the form of max-pooling, down sample the feature maps produced by the convolutional layers. Max pooling will output the maximum value in each part of a segmentation of a feature map. Pooling layers drastically reduce the computation required to train a network. They also transfer evidence of descriptors to neighboring patches, eventually allowing this information to be known globally, if the network is deep enough relative to the input image.

\subsubsection{Activation Layers}
Activation layers are used to control which nodes in a layer send output to the next layer. The standard activation currently used is the rectified linear activation unit (ReLU), which takes the form
\begin{equation}
f(x) = \begin{cases}
x & x > 0 \\
0 & x\leq 0 \\
\end{cases}
\end{equation}
Various forms of ReLU have been proposed, notably the parametric ReLU (pReLU), which adds a learnable parameter, $\alpha$, to control a slope for the negative activation domain and which was used by He et al. to achieve better than human performance for ImageNet classification.\cite{he2016deep}
\begin{equation}
f(x) = \begin{cases}
x & x > 0 \\
\alpha x & x \leq 0 \\
\end{cases}
\end{equation}

Since convolutional layers only produce feature maps on local scales, fully connected layers at the end of a CNN allow for high level features to be learned. These layers have full connections to all activated neurons from previous layers, which allow for global mixing of activated feature maps. To complete our discussion of the CNN, the loss layer specifies how a network should penalize incorrect predictions. For general tasks, typically a sigmoid cross-entropy loss is used. Regularization is also used, typically in the form of L1 or L2 weight decay. Dropout layers, which deactivate a random subset of a layer's neurons in each iteration of training, have also proven highly effective for neural network regularization by preventing networks from becoming overly reliant on certain neural paths.\cite{srivastava2014dropout}

The term deep CNN refers to a CNN that has many layers. This definition is highly variable: the popular VGG16 model has 16 layers\cite{simonyan2014very} and popular versions of ResNet contain 34, 50, 101, and 152 layers\cite{he2016deep}.

We can more precisely define a CNN as a function $f$ that takes parameters $\theta$ and an input image $I$ to produce an image embedding $x$ and a loss $L$.

\subsection{Deep Networks Today}
Recent research has shown that increasing the depth of deep neural networks increases their performance.\cite{szegedy2015going} Much work on different activation functions has allowed CNNs to become sparser and thus take up less memory, despite their increasing depth.\cite{simonyan2014very}\cite{szegedy2015going} In combination, these advancements have allowed networks that are hundreds of layers deep to become standard, and these extremely deep models have continued to excel.  In recent years, some deep learning models have achieved classification accuracies surpassing even human performance. In 2015, He et al. achieved a 4.94\% top-5 test error on the ImageNet 2012 dataset, surpassing the human error performance of 5.1\%\cite{he2016deep}. It is however, as He notes, possible to build models that are too deep. Past a certain point, accuracy saturates, and the addition of layers beyond this point of saturation leads to an increase in training error.

The features learned by deep learning models capture enough semantic meaning about images that they can be used to transfer learning from ImageNet trained models to other tasks with some success. Athiwaratkun and Kang show that just the image representation extracted by deep CNNs can be combined with simpler classifiers such as SVMs and random forests to achieve high accuracies for clustering tasks.\cite{athiwaratkun2015feature}

Importantly, in a departure from classical image processing techniques, deep learning methods do not require the manual crafting of features based on domain-level knowledge. Instead, deep learning models learn to abstract patterns from data automatically. It has been shown through visualizations of the weights learned by deep networks that these network learn many of the same low-level features as do classical techniques. Typically, the first few layers of a network will learn various edge detection filters. Many of these filters will be replicated throughout the network in slightly different fashions, which show that networks learn invariance for basic descriptors. And similarly to how much research in the 2000s focused on aggregating counts of local feature descriptors into larger parent patches of the image, many deep networks include pooling and fully connected layers, which effectively spread knowledge about locally detected features to neighboring patches of the network. 


\subsection{Image Invariants}
Ever since Krizhevsky et al. popularized the use of deep CNNs in 2012, researchers have focused on ways of incorporating various image invariants in their deep models. Invariance is learned when many examples of the same class are fed to a model, forcing the model to generalize its learned representation of that class to ignore minor differences in the image. In practice, this is often achieved through data augmentation --e.g., flipping, rotating, shearing, zooming, and changing color intensities in images. But in many cases, datasets either already contain enough images that capture certain invariants, or datasets are curated with the goal of including certain invariances. For example, Hadsell et al. learn an invariant mapping for dimensionality reduction by taking images of a plane from a full span of 360$\deg$ angles.\cite{hadsell2006dimensionality} Regardless of how invariance finds its way into the dataset, it has become standard for many tasks to verify that invariance is being learned. In demonstrating that an unsupervised model could detect the presence of high level features such as cat faces, Le et al. also use an out-of-plane rotation dataset to confirm that their model learned rotational invariance, which was particularly important since cat faces are often turned to the side.\cite{le2013building}

It is said that CNNs have translational invariance baked into their architecture. As CNNs apply kernels independently of location across input images, is it impossible for CNNs not to learn translational invariance of kernel activations. Lenc and Vedaldi's study on invariants in deep networks concluded that classification datasets contain enough examples with naturally occurring horizontal flips and rescaling for these invariants to be learned as well.\cite{vedaldi2010vlfeat} Rotational and vertical flip invariance is less commonly seen and are not usually naturally enforced.


\section{Deep Learning for Image Similarity}
\subsection{Siamese Neural Networks}

As noted by Wang et al.\cite{wang2014learning}, the network structures that are effective at classifying images into object classes are not necessarily well-designed for detecting image similarity, especially when similarity detection must be fine enough not just to identify whether two objects are in the same class, but also to rank similarities within classes. The Siamese architecture, first proposed by Bromley et al. in 1994\cite{bromley1993signature} for the purposes of verifying signatures, is well suited for this task. Siamese networks have since been used in a variety of similarity tasks, such as ground-to-aerial geolocalization by Lin\cite{lin2015learning}, matching visual similarity for product design by Bell and Bala\cite{bell2015learning}, comparing image patches\cite{zagoruyko2015learning}, and one-shot image classification by Koch\cite{koch2015siamese}.

A Siamese net is a formulation of at least two copies of a CNN that share parameters and hyperparameters, as well as a loss layer and thus weight updates. If we represent a Siamese net as $f$, then $f$ takes network weights $\theta$, two images \textbf{$I_1, I_2$} as well as an indicator variable $p$ to indicate if these images form a positive (similar) or negative (dissimilar) pair, produces two embeddings $x_1, x_2$ and one loss $L$. We wish to find a locally optimal $\theta$ such that for a triplet of embeddings $x_1, x_2, x_3$ produced by $f$, if $|x_1 - x_2|_2^2 < |x_1 - x_3|_2^2$, then we expect \textbf{$I_1$} and \textbf{$I_2$} to be much more semantically similar than \textbf{$I_1$} and \textbf{$I_3$}.

As discussed by Bell and Bala\cite{bell2015learning}, Siamese networks can be tweaked in various ways to produce both an embedding and a classification for each pair of images. The output from the CNN layer can regularized either for the embedding predictions or the class predictions or both. If there are multiple outputs from the CNN layer, multiple loss layers can be used, as well.

For the purposes of learning image similarity, only a simple formulation is required. The embeddings produced by each half of the Siamese network must be compared in some manner, and a standard Euclidean distance is usually used. Formally, we define the similarity of two images, $S(A,B)$, as the Euclidean distance of their feature embedded vectors, $f(A)$ and $f(B)$:
\begin{equation}
S(A,B) = ||f(A) - f(B)||_2^2
\end{equation}
A cosine similarity $\dfrac{f(A) \cdot f(B)}{||f(A)||||f(B)||}$ can also be used but this is less common. We note that though in the simple Siamese formula discussed here, $f(\cdot)$ is taken as the weight representation of the final convolutional block of a single CNN, we can construct a Siamese net that runs each image through mutiple CNNs or other feature extraction modules. In this case, $f(\cdot)$ would refer to the aggregate feature embedding found by all the modules. Embeddings might be aggregated via concatenation or another, more complicated technique.

We base our network design, which we discuss in \ref{chapter:network}, off of a fairly complex Siamese network with two different feature extracting modules and a blending model for their aggregation.


\section{Weakly Supervised Learning}
Deep learning usually requires massive amounts of labeled data and the problems with relying on supervised learning at a large scale are many. It is difficult and time consuming to create large datasets, and even when these are created, such as for ImageNet, they are often specific to a certain type of task and do not necessarily extend to novel concepts. As Russakowsky et al. discussed\cite{russakovsky2013detecting}, models pretrained on general datsets like ImageNet were only good at dividing images into basic classes. Russakowsky et al. also demonstrated that these models were better tuned for classification of natural classes such as animals than they were for man-made objects, suggesting that even an expansive 12 million image-large dataset like ImageNet still had flaws for general classification training. In recent years, a proliferation of intra-class datasets have allowed deep learning techniques to tackle tasks such as differentiating species of flowers\cite{angelova2013image}, leaves\cite{rejeb2013vantage}, and birds\cite{berg2014birdsnap}, but the limitations of a class-based formulation of image similarity remain apparent. More and more specific class formulas will need to be created, and this is highly reliant on how explicit human annotations are. For example, labeling an image as a dog, which might be reasonable for an animal differentiation task, will create problems if this data is ever used for a more fine-grained image similarity task, such as one that requires differentiation of Labradors from Golden Retrievers.

Much work on using weakly supervised learning has focused on using web images as a source of easily and quickly obtainable weakly labeled data.\cite{bergamo2010exploiting}\cite{fergus2010learning}\cite{li2010optimol}\cite{schroff2011harvesting}. While on the whole successful, researchers note that web-created classes suffer from polysemy and noise problems. A search for penguin images will not necessarily return only penguins because of the way images and surrounding text are indexed, and a search for screens might return both door screens and computer screens. In the field of object segmentations, Rubinstein et al. achieved better than state-of-the-art benchmark results training on a web gathered corpus, yet also noted difficulty with certain classes because of the low quality of some images.\cite{rubinstein2013unsupervised}

Krause et al. use fuzzy web-based data to remarkable effectiveness, successfully training classification for over 14,000 image classes, achieving close to human accuracy on the CUB Caltech bird species dataset, and beating state of the art results for CUB and the Stanford dog dataset\cite{krause2016unreasonable}. They quantify the fuzziness (incorrectly labeled images) in their images classes to be an average of 16\%, demonstrating that well-trained CNNs are still able to overcome significant fuzziness. It is this success that motivates the use of Flickr images, which we show in the next chapter, are even more difficult to label.

